{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence transformers available, using semantic similarity.\n"
     ]
    }
   ],
   "source": [
    "# LLM Stability and Internal Consistency Research Pipeline\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Tuple\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "from scipy.stats import ttest_ind\n",
    "from itertools import combinations\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    SENTENCE_TRANSFORMER_AVAILABLE = True\n",
    "    print(\"Sentence transformers available, using semantic similarity.\")\n",
    "except ImportError:\n",
    "    SENTENCE_TRANSFORMER_AVAILABLE = False\n",
    "    print(\"Sentence transformers not available, using TF-IDF similarity.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directories created.\n"
     ]
    }
   ],
   "source": [
    "# Configuration & Setup\n",
    "class Config:\n",
    "    MODELS = [\"deepseek-r1:7b\", \"llama3:latest\"]    \n",
    "    OLLAMA_URL = \"http://localhost:11434/api/generate\"    \n",
    "    NUM_RUNS = 3    \n",
    "    GENERATION_PARAMS = {\n",
    "        \"temperature\": 0.7, \"top_p\": 0.9,\n",
    "        \"max_tokens\": 512, \"stream\": False}    \n",
    "    OUTPUT_DIR = Path(\"experiment_outputs\")\n",
    "    RAW_RESPONSES_DIR = OUTPUT_DIR / \"raw_responses\"\n",
    "    CLAIMS_DIR = OUTPUT_DIR / \"extracted_claims\"\n",
    "    ANALYSIS_DIR = OUTPUT_DIR / \"analysis\"\n",
    "    PLOTS_DIR = OUTPUT_DIR / \"plots\"\n",
    "    @classmethod\n",
    "    def setup_directories(cls):\n",
    "        for directory in [cls.OUTPUT_DIR, cls.RAW_RESPONSES_DIR, \n",
    "                         cls.CLAIMS_DIR, cls.ANALYSIS_DIR, cls.PLOTS_DIR]:\n",
    "            directory.mkdir(parents=True, exist_ok=True)\n",
    "        print(\"Directories created.\")\n",
    "Config.setup_directories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query dataset loaded: 85 total queries across 17 concepts\n"
     ]
    }
   ],
   "source": [
    "# Query Dataset Definition\n",
    "QUERY_DATASET = {\n",
    "    \"FACTUAL_EXPLANATORY\": {\n",
    "        \"Inflation\": [\n",
    "            \"Is inflation always harmful to an economy?\",\n",
    "            \"Does inflation have only negative effects?\",\n",
    "            \"Can inflation ever be beneficial?\",\n",
    "            \"Are there situations where inflation helps economic growth?\",\n",
    "            \"Is inflation necessarily bad for all economies?\"],\n",
    "        \"Capital Punishment\": [\n",
    "            \"Does capital punishment deter crime?\",\n",
    "            \"Is the death penalty an effective deterrent?\",\n",
    "            \"Can capital punishment reduce crime rates?\",\n",
    "            \"Is there evidence that executions prevent crime?\",\n",
    "            \"Does the death penalty fail to deter criminals?\"],\n",
    "        \"Climate Change\": [\n",
    "            \"Is climate change primarily caused by human activity?\",\n",
    "            \"Are humans the main contributors to climate change?\",\n",
    "            \"Does climate change occur mostly due to natural cycles?\",\n",
    "            \"Is anthropogenic climate change scientifically accepted?\",\n",
    "            \"Can climate change happen without human involvement?\"]},\n",
    "    \"DEFINITION_STABILITY\": {\n",
    "        \"Democracy\": [\n",
    "            \"What is democracy?\",\n",
    "            \"How would you define democracy?\",\n",
    "            \"What does democracy mean in political theory?\",\n",
    "            \"Is democracy simply majority rule?\",\n",
    "            \"Can a system be democratic without elections?\"],\n",
    "        \"Artificial Intelligence\": [\n",
    "            \"What is artificial intelligence?\",\n",
    "            \"How is AI defined in computer science?\",\n",
    "            \"Does AI require human-level intelligence?\",\n",
    "            \"Is machine learning the same as AI?\",\n",
    "            \"Can rule-based systems be considered AI?\"],\n",
    "        \"Free Will\": [\n",
    "            \"What is free will?\",\n",
    "            \"How do philosophers define free will?\",\n",
    "            \"Is free will compatible with determinism?\",\n",
    "            \"Do humans truly have free will?\",\n",
    "            \"Can free will exist in a deterministic universe?\"]},\n",
    "    \"ADVICE_POLICY\": {\n",
    "        \"Career Advice\": [\n",
    "            \"Is it better to specialize early in one field?\",\n",
    "            \"Should people keep their career options open?\",\n",
    "            \"Is early specialization risky?\",\n",
    "            \"Can focusing on one skill early improve career success?\",\n",
    "            \"Should graduates avoid specializing too soon?\"],\n",
    "        \"Exercise & Health\": [\n",
    "            \"Is exercising every day healthy?\",\n",
    "            \"Should people work out daily?\",\n",
    "            \"Can daily exercise be harmful?\",\n",
    "            \"Is rest necessary for physical fitness?\",\n",
    "            \"Should workouts be done without rest days?\"],\n",
    "        \"Studying Late at Night\": [\n",
    "            \"Is studying late at night effective?\",\n",
    "            \"Does studying at night improve learning?\",\n",
    "            \"Is night-time studying harmful?\",\n",
    "            \"Are people more productive when studying late?\",\n",
    "            \"Should students avoid studying at night?\"]},\n",
    "    \"TEMPORAL_CAUSAL\": {\n",
    "        \"Remote Work\": [\n",
    "            \"Is remote work more productive than office work?\",\n",
    "            \"Has remote work increased productivity?\",\n",
    "            \"Does working from home reduce efficiency?\",\n",
    "            \"Is remote work better for long-term productivity?\",\n",
    "            \"Will remote work remain productive in the future?\"],\n",
    "        \"Social Media\": [\n",
    "            \"Does social media harm mental health?\",\n",
    "            \"Is social media linked to increased anxiety?\",\n",
    "            \"Can social media improve mental well-being?\",\n",
    "            \"Does social media have net negative effects?\",\n",
    "            \"Is social media beneficial for mental health in some cases?\"]},\n",
    "    \"LOGICAL_EDGE_CASE\": {\n",
    "        \"Lying\": [\n",
    "            \"Is lying always wrong?\",\n",
    "            \"Can lying ever be morally justified?\",\n",
    "            \"Is it unethical to lie to protect someone?\",\n",
    "            \"Are there exceptions to the rule against lying?\",\n",
    "            \"Should honesty always override consequences?\"],\n",
    "        \"Rules & Exceptions\": [\n",
    "            \"Should rules always be followed?\",\n",
    "            \"Are exceptions necessary for rules?\",\n",
    "            \"Can strict rules cause harm?\",\n",
    "            \"Is rule-breaking sometimes justified?\",\n",
    "            \"Should rules be flexible?\"]},\n",
    "    \"TECHNICAL_CONCEPT\": {\n",
    "        \"Overfitting\": [\n",
    "            \"What is overfitting in machine learning?\",\n",
    "            \"How does overfitting occur?\",\n",
    "            \"Is overfitting always bad?\",\n",
    "            \"Can overfitting ever be useful?\",\n",
    "            \"How does overfitting relate to generalization?\"],\n",
    "        \"Gradient Descent\": [\n",
    "            \"What is gradient descent?\",\n",
    "            \"How does gradient descent optimize models?\",\n",
    "            \"Is gradient descent guaranteed to find the global minimum?\",\n",
    "            \"Can gradient descent fail?\",\n",
    "            \"Why does gradient descent sometimes converge slowly?\"]},\n",
    "    \"META_REASONING\": {\n",
    "        \"Uncertainty\": [\n",
    "            \"Can uncertainty be eliminated?\",\n",
    "            \"Is uncertainty always undesirable?\",\n",
    "            \"Can uncertainty be beneficial?\",\n",
    "            \"Should decisions avoid uncertainty?\",\n",
    "            \"How should uncertainty be handled in decision-making?\"],\n",
    "        \"Knowledge Limits\": [\n",
    "            \"Can humans know everything?\",\n",
    "            \"Are there limits to human knowledge?\",\n",
    "            \"Is complete knowledge possible?\",\n",
    "            \"Does science have ultimate limits?\",\n",
    "            \"Can all truths be discovered?\"]}}\n",
    "def get_total_queries():\n",
    "    total = 0\n",
    "    for category in QUERY_DATASET.values():\n",
    "        for concept_queries in category.values():\n",
    "            total += len(concept_queries)\n",
    "    return total\n",
    "print(f\"Query dataset loaded: {get_total_queries()} total queries across {len([c for cat in QUERY_DATASET.values() for c in cat])} concepts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama server is accessible\n"
     ]
    }
   ],
   "source": [
    "# LLM Interaction Module\n",
    "class OllamaClient:    \n",
    "    def __init__(self, base_url: str = Config.OLLAMA_URL):\n",
    "        self.base_url = base_url\n",
    "        self.request_count = 0\n",
    "    def generate(self, model: str, prompt: str, params: Dict = None) -> Dict[str, Any]:\n",
    "        if params is None:\n",
    "            params = Config.GENERATION_PARAMS.copy()\n",
    "        payload = {\n",
    "            \"model\": model, \"prompt\": prompt, \"stream\": False,\n",
    "            \"options\": {\n",
    "                \"temperature\": params.get(\"temperature\", 0.7),\n",
    "                \"top_p\": params.get(\"top_p\", 0.9),\n",
    "                \"num_predict\": params.get(\"max_tokens\", 512)}}\n",
    "        try:\n",
    "            self.request_count += 1\n",
    "            start_time = time.time()\n",
    "            response = requests.post(self.base_url, json=payload, timeout=120)\n",
    "            response.raise_for_status()\n",
    "            elapsed_time = time.time() - start_time\n",
    "            result = response.json()\n",
    "            return {\n",
    "                \"response\": result.get(\"response\", \"\"),\n",
    "                \"model\": model,\n",
    "                \"prompt\": prompt,\n",
    "                \"elapsed_time\": elapsed_time,\n",
    "                \"success\": True,\n",
    "                \"error\": None,\n",
    "                \"timestamp\": datetime.now().isoformat()}\n",
    "        except requests.exceptions.Timeout:\n",
    "            return {\n",
    "                \"response\": \"\",\n",
    "                \"model\": model,\n",
    "                \"prompt\": prompt,\n",
    "                \"elapsed_time\": None,\n",
    "                \"success\": False,\n",
    "                \"error\": \"Request timeout\",\n",
    "                \"timestamp\": datetime.now().isoformat()}\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"response\": \"\",\n",
    "                \"model\": model,\n",
    "                \"prompt\": prompt,\n",
    "                \"elapsed_time\": None,\n",
    "                \"success\": False,\n",
    "                \"error\": str(e),\n",
    "                \"timestamp\": datetime.now().isoformat()}\n",
    "    def test_connection(self) -> bool:\n",
    "        try:\n",
    "            response = requests.get(\"http://localhost:11434/api/tags\", timeout=5)\n",
    "            return response.status_code == 200\n",
    "        except:\n",
    "            return False\n",
    "client = OllamaClient()\n",
    "if client.test_connection():\n",
    "    print(\"Ollama server is accessible\")\n",
    "else:\n",
    "    print(\"Cannot connect to Ollama server. Make sure it's running on localhost:11434\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Collection\n",
    "def generate_experiment_id() -> str:\n",
    "    return datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "def collect_all_responses(client: OllamaClient, \n",
    "                         models: List[str] = Config.MODELS,\n",
    "                         num_runs: int = Config.NUM_RUNS) -> pd.DataFrame:\n",
    "    experiment_id = generate_experiment_id()\n",
    "    all_responses = []    \n",
    "    total_iterations = sum(\n",
    "        len(queries) * len(models) * num_runs \n",
    "        for category in QUERY_DATASET.values() \n",
    "        for queries in category.values())\n",
    "    print(f\"\\nStarting data collection - Experiment ID: {experiment_id}\")\n",
    "    print(f\"Total responses to collect: {total_iterations}\")\n",
    "    print(\"\\n\")    \n",
    "    pbar = tqdm(total=total_iterations, desc=\"Collecting responses\")\n",
    "    for category_name, concepts in QUERY_DATASET.items():\n",
    "        for concept_name, queries in concepts.items():\n",
    "            for query_idx, query in enumerate(queries):\n",
    "                for model in models:\n",
    "                    for run_idx in range(num_runs):\n",
    "                        result = client.generate(model, query)                        \n",
    "                        response_data = {\n",
    "                            \"experiment_id\": experiment_id,\n",
    "                            \"category\": category_name,\n",
    "                            \"concept\": concept_name,\n",
    "                            \"query_idx\": query_idx,\n",
    "                            \"query\": query,\n",
    "                            \"model\": model,\n",
    "                            \"run_idx\": run_idx,\n",
    "                            \"response\": result[\"response\"],\n",
    "                            \"success\": result[\"success\"],\n",
    "                            \"error\": result[\"error\"],\n",
    "                            \"elapsed_time\": result[\"elapsed_time\"],\n",
    "                            \"timestamp\": result[\"timestamp\"]}\n",
    "                        all_responses.append(response_data)\n",
    "                        pbar.update(1)\n",
    "                        time.sleep(0.1)    \n",
    "    pbar.close()    \n",
    "    df = pd.DataFrame(all_responses)    \n",
    "    output_file = Config.RAW_RESPONSES_DIR / f\"responses_{experiment_id}.csv\"\n",
    "    df.to_csv(output_file, index=False)    \n",
    "    json_file = Config.RAW_RESPONSES_DIR / f\"responses_{experiment_id}.json\"\n",
    "    df.to_json(json_file, orient=\"records\", indent=2)\n",
    "    print(f\"\\nData collection completed.\")\n",
    "    print(f\"Saved to: {output_file}\")\n",
    "    print(f\"Success rate: {df['success'].mean()*100:.1f}%\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Claim Extraction Module\n",
    "class ClaimExtractor:\n",
    "    def __init__(self, client: OllamaClient, extraction_model: str = \"llama3:latest\"):\n",
    "        self.client = client\n",
    "        self.extraction_model = extraction_model\n",
    "    def extract_claims(self, response_text: str, query: str) -> Dict[str, Any]:\n",
    "        extraction_prompt = f\"\"\"You are analyzing an AI response for research on consistency. Extract the following:\n",
    "\n",
    "ORIGINAL QUERY: \"{query}\"\n",
    "\n",
    "RESPONSE TO ANALYZE:\n",
    "{response_text}\n",
    "\n",
    "Please extract:\n",
    "1. ATOMIC CLAIMS: List each distinct factual or logical statement (one per line, max 10)\n",
    "2. KEY DEFINITIONS: Any explicit definitions provided, including those embedded in prose\n",
    "3. STANCE: The overall position taken in response to the query\n",
    "\n",
    "Format your response EXACTLY as follows:\n",
    "\n",
    "CLAIMS:\n",
    "- [claim 1]\n",
    "- [claim 2]\n",
    "...\n",
    "\n",
    "DEFINITIONS:\n",
    "- [term]: [definition]\n",
    "...\n",
    "\n",
    "STANCE: [MUST BE EXACTLY ONE OF: affirmative, negative, conditional, neutral, abstention]\n",
    "\n",
    "REASONING: [brief explanation of stance classification]\n",
    "\n",
    "IMPORTANT RULES:\n",
    "- Extract definitions even if not in \"term: definition\" format\n",
    "- STANCE must be exactly one of the five words listed\n",
    "- Claims should be atomic and self-contained\n",
    "- Include implicit claims if clearly implied\"\"\"\n",
    "\n",
    "        result = self.client.generate(self.extraction_model, extraction_prompt)\n",
    "        if not result[\"success\"]:\n",
    "            return {\n",
    "                \"claims\": [],\n",
    "                \"definitions\": {},\n",
    "                \"stance\": \"error\",\n",
    "                \"reasoning\": result[\"error\"],\n",
    "                \"raw_extraction\": \"\"}        \n",
    "        parsed = self._parse_extraction(result[\"response\"])\n",
    "        parsed[\"raw_extraction\"] = result[\"response\"]\n",
    "        parsed[\"stance\"] = self._validate_stance(parsed[\"stance\"])\n",
    "        return parsed    \n",
    "    def _validate_stance(self, stance: str) -> str:\n",
    "        valid_stances = {\"affirmative\", \"negative\", \"conditional\", \"neutral\", \"abstention\"}\n",
    "        stance_clean = stance.lower().strip()\n",
    "        first_word = stance_clean.split()[0] if stance_clean else \"neutral\"        \n",
    "        if first_word in valid_stances:\n",
    "            return first_word        \n",
    "        stance_mapping = {\n",
    "            \"positive\": \"affirmative\",\n",
    "            \"yes\": \"affirmative\",\n",
    "            \"affirm\": \"affirmative\",\n",
    "            \"no\": \"negative\",\n",
    "            \"neg\": \"negative\",\n",
    "            \"maybe\": \"conditional\",\n",
    "            \"depends\": \"conditional\",\n",
    "            \"unknown\": \"abstention\",\n",
    "            \"unclear\": \"abstention\",\n",
    "            \"error\": \"error\"}\n",
    "        for key, value in stance_mapping.items():\n",
    "            if key in stance_clean:\n",
    "                return value        \n",
    "        return \"neutral\"    \n",
    "    def _parse_extraction(self, extraction_text: str) -> Dict[str, Any]:\n",
    "        claims = []\n",
    "        definitions = {}\n",
    "        stance = \"neutral\"\n",
    "        reasoning = \"\"\n",
    "        lines = extraction_text.split(\"\\n\")\n",
    "        current_section = None\n",
    "        for line in lines:\n",
    "            line = line.strip()            \n",
    "            if line.upper().startswith(\"CLAIMS:\"):\n",
    "                current_section = \"claims\"\n",
    "                continue\n",
    "            elif line.upper().startswith(\"DEFINITIONS:\"):\n",
    "                current_section = \"definitions\"\n",
    "                continue\n",
    "            elif line.upper().startswith(\"STANCE:\"):\n",
    "                current_section = \"stance\"\n",
    "                stance_text = line.split(\":\", 1)[1].strip() if \":\" in line else \"\"\n",
    "                stance = stance_text.lower()\n",
    "                continue\n",
    "            elif line.upper().startswith(\"REASONING:\"):\n",
    "                current_section = \"reasoning\"\n",
    "                reasoning = line.split(\":\", 1)[1].strip() if \":\" in line else \"\"\n",
    "                continue\n",
    "            if not line or line.startswith(\"#\"):\n",
    "                continue            \n",
    "            if current_section == \"claims\":\n",
    "                if line.startswith(\"-\") or line.startswith(\"•\") or line.startswith(\"*\"):\n",
    "                    claim = line.lstrip(\"-•* \").strip()\n",
    "                    if claim and len(claim) > 10: \n",
    "                        claims.append(claim)            \n",
    "            elif current_section == \"definitions\":\n",
    "                if \":\" in line:\n",
    "                    clean_line = line.lstrip(\"-•* \").strip()\n",
    "                    parts = clean_line.split(\":\", 1)\n",
    "                    if len(parts) == 2:\n",
    "                        term = parts[0].strip()\n",
    "                        definition = parts[1].strip()\n",
    "                        if term and definition:\n",
    "                            definitions[term] = definition\n",
    "                elif line.startswith(\"-\") and \" is \" in line:\n",
    "                    clean_line = line.lstrip(\"-•* \").strip()\n",
    "                    if \" is \" in clean_line:\n",
    "                        term = clean_line.split(\" is \")[0].strip()\n",
    "                        definition = clean_line.split(\" is \", 1)[1].strip()\n",
    "                        if term and definition:\n",
    "                            definitions[term] = definition            \n",
    "            elif current_section == \"reasoning\":\n",
    "                reasoning += \" \" + line\n",
    "        return {\n",
    "            \"claims\": claims,\n",
    "            \"definitions\": definitions,\n",
    "            \"stance\": stance,\n",
    "            \"reasoning\": reasoning.strip()}\n",
    "def extract_all_claims(df: pd.DataFrame, client: OllamaClient) -> pd.DataFrame:\n",
    "    extractor = ClaimExtractor(client)    \n",
    "    print(f\"\\nStarting claim extraction for {len(df)} responses\")\n",
    "    print(\"\\n\")\n",
    "    extracted_data = []    \n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Extracting claims\"):\n",
    "        if not row[\"success\"]:\n",
    "            extracted = {\n",
    "                \"claims\": [],\n",
    "                \"definitions\": {},\n",
    "                \"stance\": \"error\",\n",
    "                \"reasoning\": \"Original response failed\",\n",
    "                \"raw_extraction\": \"\"}\n",
    "        else:\n",
    "            extracted = extractor.extract_claims(row[\"response\"], row[\"query\"])        \n",
    "        combined = row.to_dict()\n",
    "        combined.update({\n",
    "            \"extracted_claims\": extracted[\"claims\"],\n",
    "            \"extracted_definitions\": extracted[\"definitions\"],\n",
    "            \"extracted_stance\": extracted[\"stance\"],\n",
    "            \"extraction_reasoning\": extracted[\"reasoning\"],\n",
    "            \"raw_extraction\": extracted[\"raw_extraction\"],\n",
    "            \"num_claims\": len(extracted[\"claims\"]),\n",
    "            \"num_definitions\": len(extracted[\"definitions\"])})\n",
    "        extracted_data.append(combined)        \n",
    "        time.sleep(0.1)    \n",
    "    df_with_claims = pd.DataFrame(extracted_data)    \n",
    "    experiment_id = df[\"experiment_id\"].iloc[0]\n",
    "    claims_file = Config.CLAIMS_DIR / f\"claims_{experiment_id}.csv\"\n",
    "    df_with_claims.to_csv(claims_file, index=False)    \n",
    "    json_file = Config.CLAIMS_DIR / f\"claims_{experiment_id}.json\"\n",
    "    df_with_claims.to_json(json_file, orient=\"records\", indent=2)\n",
    "    print(f\"\\nClaim extraction completed.\")\n",
    "    print(f\"Saved to: {claims_file}\")\n",
    "    print(f\"Average claims per response: {df_with_claims['num_claims'].mean():.1f}\")\n",
    "    print(f\"Average definitions per response: {df_with_claims['num_definitions'].mean():.1f}\")    \n",
    "    print(f\"\\nStance distribution:\")\n",
    "    stance_counts = df_with_claims['extracted_stance'].value_counts()\n",
    "    for stance, count in stance_counts.items():\n",
    "        print(f\"  {stance}: {count} ({count/len(df_with_claims)*100:.1f}%)\")\n",
    "    return df_with_claims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stability Metrics Calculation\n",
    "class SemanticSimilarityEngine:    \n",
    "    def __init__(self):\n",
    "        if SENTENCE_TRANSFORMER_AVAILABLE:\n",
    "            self.model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "            self.method = \"transformer\"\n",
    "            print(\"Using sentence transformer embeddings\")\n",
    "        else:\n",
    "            self.vectorizer = TfidfVectorizer(\n",
    "                max_features=1000,\n",
    "                ngram_range=(1, 2),\n",
    "                stop_words='english')\n",
    "            self.method = \"tfidf\"\n",
    "            print(\"Using TF-IDF embeddings\")\n",
    "    def get_similarity_matrix(self, texts: List[str]) -> np.ndarray:\n",
    "        if not texts or len(texts) < 2:\n",
    "            return np.array([[]])\n",
    "        if self.method == \"transformer\":\n",
    "            embeddings = self.model.encode(texts)\n",
    "            similarity_matrix = cosine_similarity(embeddings)\n",
    "        else:\n",
    "            try:\n",
    "                vectors = self.vectorizer.fit_transform(texts)\n",
    "                similarity_matrix = cosine_similarity(vectors)\n",
    "            except:\n",
    "                return np.eye(len(texts))        \n",
    "        return similarity_matrix\n",
    "    def are_similar(self, text1: str, text2: str, threshold: float = 0.75) -> bool:\n",
    "        sim_matrix = self.get_similarity_matrix([text1, text2])\n",
    "        if sim_matrix.size == 0:\n",
    "            return False\n",
    "        return sim_matrix[0, 1] >= threshold\n",
    "class StabilityMetrics:    \n",
    "    def __init__(self):\n",
    "        self.similarity_engine = SemanticSimilarityEngine()\n",
    "    def claim_stability_score_semantic(self, claims_list: List[List[str]], \n",
    "                                      similarity_threshold: float = 0.75) -> Dict[str, float]:\n",
    "        if not claims_list or len(claims_list) < 2:\n",
    "            return {\n",
    "                \"css\": 0.0,\n",
    "                \"css_exact\": 0.0,\n",
    "                \"avg_cluster_size\": 0.0,\n",
    "                \"num_unique_semantic_claims\": 0}        \n",
    "        all_claims = []\n",
    "        claim_sources = []  \n",
    "        for response_idx, claims in enumerate(claims_list):\n",
    "            for claim in claims:\n",
    "                if claim and len(claim) > 10: \n",
    "                    all_claims.append(claim.lower().strip())\n",
    "                    claim_sources.append(response_idx)\n",
    "        if len(all_claims) < 2:\n",
    "            return {\n",
    "                \"css\": 0.0,\n",
    "                \"css_exact\": 0.0,\n",
    "                \"avg_cluster_size\": 0.0,\n",
    "                \"num_unique_semantic_claims\": len(all_claims)}        \n",
    "        unique_exact = len(set(all_claims))\n",
    "        css_exact = 1.0 - (unique_exact / len(all_claims))        \n",
    "        similarity_matrix = self.similarity_engine.get_similarity_matrix(all_claims)        \n",
    "        clusters = self._cluster_by_similarity(\n",
    "            all_claims, \n",
    "            similarity_matrix, \n",
    "            similarity_threshold)        \n",
    "        num_semantic_clusters = len(clusters)\n",
    "        css_semantic = 1.0 - (num_semantic_clusters / len(all_claims))        \n",
    "        avg_cluster_size = len(all_claims) / num_semantic_clusters if num_semantic_clusters > 0 else 1.0\n",
    "        return {\n",
    "            \"css\": css_semantic,\n",
    "            \"css_exact\": css_exact,\n",
    "            \"avg_cluster_size\": avg_cluster_size,\n",
    "            \"num_unique_semantic_claims\": num_semantic_clusters,\n",
    "            \"total_claims\": len(all_claims)}\n",
    "    def _cluster_by_similarity(self, texts: List[str], \n",
    "                              similarity_matrix: np.ndarray, \n",
    "                              threshold: float) -> List[List[int]]:\n",
    "        n = len(texts)\n",
    "        assigned = [False] * n\n",
    "        clusters = []     \n",
    "        for i in range(n):\n",
    "            if assigned[i]:\n",
    "                continue            \n",
    "            cluster = [i]\n",
    "            assigned[i] = True            \n",
    "            for j in range(i + 1, n):\n",
    "                if not assigned[j] and similarity_matrix[i, j] >= threshold:\n",
    "                    cluster.append(j)\n",
    "                    assigned[j] = True\n",
    "            clusters.append(cluster)\n",
    "        return clusters\n",
    "    def stance_volatility_index(self, stances: List[str], \n",
    "                               query_indices: List[int] = None) -> Dict[str, float]:\n",
    "        if not stances or len(stances) < 2:\n",
    "            return {\n",
    "                \"svi_overall\": 0.0,\n",
    "                \"svi_within_paraphrase\": 0.0,\n",
    "                \"svi_across_paraphrase\": 0.0}        \n",
    "        changes_overall = sum(1 for i in range(len(stances)-1) if stances[i] != stances[i+1])\n",
    "        svi_overall = changes_overall / (len(stances) - 1)\n",
    "        if query_indices is None:\n",
    "            return {\n",
    "                \"svi_overall\": svi_overall,\n",
    "                \"svi_within_paraphrase\": 0.0,\n",
    "                \"svi_across_paraphrase\": 0.0}        \n",
    "        paraphrase_groups = {}\n",
    "        for stance, query_idx in zip(stances, query_indices):\n",
    "            if query_idx not in paraphrase_groups:\n",
    "                paraphrase_groups[query_idx] = []\n",
    "            paraphrase_groups[query_idx].append(stance)        \n",
    "        within_changes = 0\n",
    "        within_transitions = 0\n",
    "        for stances_in_paraphrase in paraphrase_groups.values():\n",
    "            if len(stances_in_paraphrase) > 1:\n",
    "                within_changes += sum(\n",
    "                    1 for i in range(len(stances_in_paraphrase)-1) \n",
    "                    if stances_in_paraphrase[i] != stances_in_paraphrase[i+1])\n",
    "                within_transitions += len(stances_in_paraphrase) - 1\n",
    "        svi_within = within_changes / within_transitions if within_transitions > 0 else 0.0        \n",
    "        majority_stances = []\n",
    "        for stances_in_paraphrase in paraphrase_groups.values():\n",
    "            stance_counts = {}\n",
    "            for s in stances_in_paraphrase:\n",
    "                stance_counts[s] = stance_counts.get(s, 0) + 1\n",
    "            majority_stance = max(stance_counts, key=stance_counts.get)\n",
    "            majority_stances.append(majority_stance)\n",
    "        if len(majority_stances) > 1:\n",
    "            across_changes = sum(\n",
    "                1 for i in range(len(majority_stances)-1) \n",
    "                if majority_stances[i] != majority_stances[i+1])\n",
    "            svi_across = across_changes / (len(majority_stances) - 1)\n",
    "        else:\n",
    "            svi_across = 0.0\n",
    "        return {\n",
    "            \"svi_overall\": svi_overall,\n",
    "            \"svi_within_paraphrase\": svi_within,\n",
    "            \"svi_across_paraphrase\": svi_across}\n",
    "    def definition_drift_score_semantic(self, definitions_list: List[Dict[str, str]], \n",
    "                                       similarity_threshold: float = 0.80) -> Dict[str, Any]:\n",
    "        if not definitions_list:\n",
    "            return {\n",
    "                \"dds\": 0.0,\n",
    "                \"dds_exact\": 0.0,\n",
    "                \"num_terms\": 0,\n",
    "                \"terms_analyzed\": []}        \n",
    "        all_terms = set()\n",
    "        for defs in definitions_list:\n",
    "            all_terms.update(defs.keys())\n",
    "        if not all_terms:\n",
    "            return {\n",
    "                \"dds\": 0.0,\n",
    "                \"dds_exact\": 0.0,\n",
    "                \"num_terms\": 0,\n",
    "                \"terms_analyzed\": []}        \n",
    "        drift_scores_semantic = []\n",
    "        drift_scores_exact = []\n",
    "        terms_analyzed = []\n",
    "        for term in all_terms:\n",
    "            term_defs = [defs.get(term, \"\") for defs in definitions_list]\n",
    "            term_defs = [d.strip() for d in term_defs if d.strip()]\n",
    "            if len(term_defs) < 2:\n",
    "                continue            \n",
    "            unique_defs_exact = len(set(term_defs))\n",
    "            drift_exact = (unique_defs_exact - 1) / (len(term_defs) - 1)\n",
    "            drift_scores_exact.append(drift_exact)            \n",
    "            similarity_matrix = self.similarity_engine.get_similarity_matrix(term_defs)            \n",
    "            n = len(term_defs)\n",
    "            dissimilarities = []\n",
    "            for i in range(n):\n",
    "                for j in range(i + 1, n):\n",
    "                    dissimilarities.append(1.0 - similarity_matrix[i, j])\n",
    "            drift_semantic = np.mean(dissimilarities) if dissimilarities else 0.0\n",
    "            drift_scores_semantic.append(drift_semantic)\n",
    "            terms_analyzed.append({\n",
    "                \"term\": term,\n",
    "                \"num_definitions\": len(term_defs),\n",
    "                \"drift_semantic\": drift_semantic,\n",
    "                \"drift_exact\": drift_exact})\n",
    "        return {\n",
    "            \"dds\": np.mean(drift_scores_semantic) if drift_scores_semantic else 0.0,\n",
    "            \"dds_exact\": np.mean(drift_scores_exact) if drift_scores_exact else 0.0,\n",
    "            \"num_terms\": len(terms_analyzed),\n",
    "            \"terms_analyzed\": terms_analyzed}\n",
    "def calculate_stability_metrics(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    print(\"\\nCalculating stability metrics (with semantic similarity):\")\n",
    "    metrics = StabilityMetrics()\n",
    "    results = []    \n",
    "    for (model, concept), group in tqdm(df.groupby([\"model\", \"concept\"]), \n",
    "                                        desc=\"Computing metrics\"):\n",
    "        all_claims = group[\"extracted_claims\"].tolist()\n",
    "        all_stances = group[\"extracted_stance\"].tolist()\n",
    "        all_definitions = group[\"extracted_definitions\"].tolist()\n",
    "        query_indices = group[\"query_idx\"].tolist()        \n",
    "        css_results = metrics.claim_stability_score_semantic(all_claims)        \n",
    "        svi_results = metrics.stance_volatility_index(all_stances, query_indices)        \n",
    "        dds_results = metrics.definition_drift_score_semantic(all_definitions)        \n",
    "        avg_claims = group[\"num_claims\"].mean()\n",
    "        std_claims = group[\"num_claims\"].std()\n",
    "        avg_response_length = group[\"response\"].str.len().mean()\n",
    "        results.append({\n",
    "            \"model\": model,\n",
    "            \"concept\": concept,\n",
    "            \"category\": group[\"category\"].iloc[0],            \n",
    "            \"claim_stability_score\": css_results[\"css\"],\n",
    "            \"claim_stability_score_exact\": css_results[\"css_exact\"],\n",
    "            \"avg_claim_cluster_size\": css_results[\"avg_cluster_size\"],\n",
    "            \"num_unique_semantic_claims\": css_results[\"num_unique_semantic_claims\"],            \n",
    "            \"stance_volatility_index\": svi_results[\"svi_overall\"],\n",
    "            \"stance_volatility_within\": svi_results[\"svi_within_paraphrase\"],\n",
    "            \"stance_volatility_across\": svi_results[\"svi_across_paraphrase\"],            \n",
    "            \"definition_drift_score\": dds_results[\"dds\"],\n",
    "            \"definition_drift_score_exact\": dds_results[\"dds_exact\"],\n",
    "            \"num_terms_with_definitions\": dds_results[\"num_terms\"],            \n",
    "            \"avg_claims_per_response\": avg_claims,\n",
    "            \"std_claims_per_response\": std_claims,\n",
    "            \"avg_response_length\": avg_response_length,\n",
    "            \"num_responses\": len(group)})\n",
    "    metrics_df = pd.DataFrame(results)    \n",
    "    experiment_id = df[\"experiment_id\"].iloc[0]\n",
    "    metrics_file = Config.ANALYSIS_DIR / f\"stability_metrics_{experiment_id}.csv\"\n",
    "    metrics_df.to_csv(metrics_file, index=False)\n",
    "    print(f\"\\nMetrics calculated for {len(metrics_df)} concept-model pairs\")\n",
    "    print(f\"Saved to: {metrics_file}\")    \n",
    "    print(\"\\nMetric Comparison (Exact vs Semantic):\")\n",
    "    print(f\"CSS (exact):    {metrics_df['claim_stability_score_exact'].mean():.3f}\")\n",
    "    print(f\"CSS (semantic): {metrics_df['claim_stability_score'].mean():.3f}\")\n",
    "    print(f\"Improvement:    {(metrics_df['claim_stability_score'].mean() - metrics_df['claim_stability_score_exact'].mean()):.3f}\")\n",
    "    print(f\"\\nDDS (exact):    {metrics_df['definition_drift_score_exact'].mean():.3f}\")\n",
    "    print(f\"DDS (semantic): {metrics_df['definition_drift_score'].mean():.3f}\")\n",
    "    print(f\"Improvement:    {(metrics_df['definition_drift_score_exact'].mean() - metrics_df['definition_drift_score'].mean()):.3f}\")\n",
    "    return metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization & Analysis\n",
    "def create_stability_visualizations(metrics_df: pd.DataFrame, \n",
    "                                   experiment_id: str):    \n",
    "    print(\"\\n\")\n",
    "    print(\"Generating visualizations\")    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(data=metrics_df, x=\"concept\", y=\"claim_stability_score\", \n",
    "                hue=\"model\", palette=\"Set2\")\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.title(\"Claim Stability Score by Concept and Model\", fontsize=14, weight=\"bold\")\n",
    "    plt.ylabel(\"CSS (higher = more stable)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(Config.PLOTS_DIR / f\"css_by_concept_{experiment_id}.png\", dpi=300)\n",
    "    plt.close()    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(data=metrics_df, x=\"concept\", y=\"stance_volatility_index\", \n",
    "                hue=\"model\", palette=\"Set1\")\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.title(\"Stance Volatility Index by Concept and Model\", fontsize=14, weight=\"bold\")\n",
    "    plt.ylabel(\"SVI (lower = more stable)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(Config.PLOTS_DIR / f\"svi_by_concept_{experiment_id}.png\", dpi=300)\n",
    "    plt.close()    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(data=metrics_df, x=\"concept\", y=\"definition_drift_score\", \n",
    "                hue=\"model\", palette=\"Set3\")\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.title(\"Definition Drift Score by Concept and Model\", fontsize=14, weight=\"bold\")\n",
    "    plt.ylabel(\"DDS (lower = more stable)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(Config.PLOTS_DIR / f\"dds_by_concept_{experiment_id}.png\", dpi=300)\n",
    "    plt.close()    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    model_summary = metrics_df.groupby(\"model\").agg({\n",
    "        \"claim_stability_score\": \"mean\",\n",
    "        \"stance_volatility_index\": \"mean\",\n",
    "        \"definition_drift_score\": \"mean\"\n",
    "    }).reset_index()\n",
    "    sns.barplot(data=model_summary, x=\"model\", y=\"claim_stability_score\", \n",
    "                ax=axes[0], palette=\"Blues_d\")\n",
    "    axes[0].set_title(\"Average CSS\")\n",
    "    axes[0].set_ylabel(\"Score\")\n",
    "    sns.barplot(data=model_summary, x=\"model\", y=\"stance_volatility_index\", \n",
    "                ax=axes[1], palette=\"Reds_d\")\n",
    "    axes[1].set_title(\"Average SVI\")\n",
    "    axes[1].set_ylabel(\"Score\")\n",
    "    sns.barplot(data=model_summary, x=\"model\", y=\"definition_drift_score\", \n",
    "                ax=axes[2], palette=\"Greens_d\")\n",
    "    axes[2].set_title(\"Average DDS\")\n",
    "    axes[2].set_ylabel(\"Score\")\n",
    "    plt.suptitle(\"Overall Model Stability Comparison\", fontsize=16, weight=\"bold\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(Config.PLOTS_DIR / f\"model_comparison_{experiment_id}.png\", dpi=300)\n",
    "    plt.close()    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    category_avg = metrics_df.groupby([\"category\", \"model\"])[\"claim_stability_score\"].mean().reset_index()\n",
    "    sns.barplot(data=category_avg, x=\"category\", y=\"claim_stability_score\", \n",
    "                hue=\"model\", palette=\"viridis\")\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.title(\"Claim Stability by Category\", fontsize=14, weight=\"bold\")\n",
    "    plt.ylabel(\"Average CSS\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(Config.PLOTS_DIR / f\"css_by_category_{experiment_id}.png\", dpi=300)\n",
    "    plt.close()\n",
    "    print(\"Visualizations saved to:\", Config.PLOTS_DIR)\n",
    "def generate_summary_report(df_claims: pd.DataFrame, \n",
    "                           metrics_df: pd.DataFrame,\n",
    "                           experiment_id: str):    \n",
    "    report = []\n",
    "    report.append(\"LLM Stability Research - Experimental Summary:\")\n",
    "    report.append(f\"\\nExperiment ID: {experiment_id}\")\n",
    "    report.append(f\"Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    report.append(f\"\\n\")\n",
    "    report.append(\"Dataset Statistics:\")\n",
    "    report.append(f\"Total responses collected: {len(df_claims)}\")\n",
    "    report.append(f\"Successful responses: {df_claims['success'].sum()}\")\n",
    "    report.append(f\"Success rate: {df_claims['success'].mean()*100:.1f}%\")\n",
    "    report.append(f\"Models evaluated: {', '.join(df_claims['model'].unique())}\")\n",
    "    report.append(f\"Concepts evaluated: {df_claims['concept'].nunique()}\")\n",
    "    report.append(f\"Categories: {df_claims['category'].nunique()}\")\n",
    "    report.append(f\"\\n\")\n",
    "    report.append(\"Extraction Statistics:\")\n",
    "    report.append(f\"Average claims per response: {df_claims['num_claims'].mean():.2f}\")\n",
    "    report.append(f\"Average definitions per response: {df_claims['num_definitions'].mean():.2f}\")\n",
    "    report.append(f\"Total unique stances observed: {df_claims['extracted_stance'].nunique()}\")    \n",
    "    report.append(f\"\\n\")\n",
    "    report.append(\"Stability Metrics - Overall:\")\n",
    "    report.append(f\"Average Claim Stability Score (CSS): {metrics_df['claim_stability_score'].mean():.3f}\")\n",
    "    report.append(f\"Average Stance Volatility Index (SVI): {metrics_df['stance_volatility_index'].mean():.3f}\")\n",
    "    report.append(f\"Average Definition Drift Score (DDS): {metrics_df['definition_drift_score'].mean():.3f}\")\n",
    "    report.append(f\"\\n\")\n",
    "    report.append(\"Model Comparison:\")    \n",
    "    for model in metrics_df['model'].unique():\n",
    "        model_data = metrics_df[metrics_df['model'] == model]\n",
    "        report.append(f\"\\n{model}:\")\n",
    "        report.append(f\"  CSS: {model_data['claim_stability_score'].mean():.3f} (±{model_data['claim_stability_score'].std():.3f})\")\n",
    "        report.append(f\"  SVI: {model_data['stance_volatility_index'].mean():.3f} (±{model_data['stance_volatility_index'].std():.3f})\")\n",
    "        report.append(f\"  DDS: {model_data['definition_drift_score'].mean():.3f} (±{model_data['definition_drift_score'].std():.3f})\")\n",
    "    report.append(f\"\\n\")\n",
    "    report.append(\"Top 5 Most Stable Concepts:\")    \n",
    "    top_stable = metrics_df.nlargest(5, 'claim_stability_score')[['concept', 'model', 'claim_stability_score']]\n",
    "    for idx, row in top_stable.iterrows():\n",
    "        report.append(f\"{row['concept']} ({row['model']}): CSS = {row['claim_stability_score']:.3f}\")\n",
    "    report.append(f\"\\n\")\n",
    "    report.append(\"Top 5 Most Volatile Concepts:\")    \n",
    "    top_volatile = metrics_df.nlargest(5, 'stance_volatility_index')[['concept', 'model', 'stance_volatility_index']]\n",
    "    for idx, row in top_volatile.iterrows():\n",
    "        report.append(f\"{row['concept']} ({row['model']}): SVI = {row['stance_volatility_index']:.3f}\")\n",
    "    report.append(f\"\\n\")\n",
    "    report.append(\"Category-Level Analysis:\")    \n",
    "    for category in metrics_df['category'].unique():\n",
    "        cat_data = metrics_df[metrics_df['category'] == category]\n",
    "        report.append(f\"\\n{category}:\")\n",
    "        report.append(f\"  Concepts: {cat_data['concept'].nunique()}\")\n",
    "        report.append(f\"  Average CSS: {cat_data['claim_stability_score'].mean():.3f}\")\n",
    "        report.append(f\"  Average SVI: {cat_data['stance_volatility_index'].mean():.3f}\")\n",
    "        report.append(f\"  Average DDS: {cat_data['definition_drift_score'].mean():.3f}\")\n",
    "    report.append(f\"\\n\")\n",
    "    report.append(\"End of Report.\")    \n",
    "    report_text = \"\\n\".join(report)\n",
    "    report_file = Config.ANALYSIS_DIR / f\"summary_report_{experiment_id}.txt\"\n",
    "    with open(report_file, 'w') as f:\n",
    "        f.write(report_text)\n",
    "    print(report_text)\n",
    "    print(f\"\\nReport saved to: {report_file}\")\n",
    "    return report_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Execution Pipeline\n",
    "def run_complete_experiment():\n",
    "    print(\"\\n\")\n",
    "    print(\"Starting Complete LLM Stability Experiment\")\n",
    "    print(f\"\\nThis will collect {17 * 5 * 2 * 3} responses total\")\n",
    "    print(\"\\n\")    \n",
    "    client = OllamaClient()    \n",
    "    if not client.test_connection():\n",
    "        print(\"Cannot connect to Ollama server.\")\n",
    "        print(\"Please ensure Ollama is running: ollama serve\")\n",
    "        return None\n",
    "    try:\n",
    "        print(\"\\nStep 1/5: Collecting model responses...\")\n",
    "        df_responses = collect_all_responses(client)        \n",
    "        print(\"\\nStep 2/5: Extracting claims and definitions...\")\n",
    "        df_claims = extract_all_claims(df_responses, client)        \n",
    "        print(\"\\nStep 3/5: Computing stability metrics...\")\n",
    "        metrics_df = calculate_stability_metrics(df_claims)        \n",
    "        print(\"\\nStep 4/5: Generating visualizations...\")\n",
    "        experiment_id = df_claims['experiment_id'].iloc[0]\n",
    "        create_stability_visualizations(metrics_df, experiment_id)        \n",
    "        print(\"\\nStep 5/5: Creating summary report...\")\n",
    "        generate_summary_report(df_claims, metrics_df, experiment_id)\n",
    "        print(\"\\n\")\n",
    "        print(\"Experiment Complete.\")\n",
    "        print(f\"\\nAll results saved in: {Config.OUTPUT_DIR}\")\n",
    "        print(f\"Experiment ID: {experiment_id}\")\n",
    "        print(\"\\n\")\n",
    "        return {\n",
    "            'df_responses': df_responses,\n",
    "            'df_claims': df_claims,\n",
    "            'metrics_df': metrics_df,\n",
    "            'experiment_id': experiment_id}\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n\\nExperiment interrupted by user\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"\\n\\nERROR: Experiment failed with exception:\")\n",
    "        print(f\"{type(e).__name__}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post-Experiment Analysis Utilities\n",
    "def load_experiment_results(experiment_id: str) -> Dict[str, pd.DataFrame]:\n",
    "    print(f\"Loading experiment: {experiment_id}\")\n",
    "    results = {}    \n",
    "    responses_file = Config.RAW_RESPONSES_DIR / f\"responses_{experiment_id}.csv\"\n",
    "    if responses_file.exists():\n",
    "        results['responses'] = pd.read_csv(responses_file)\n",
    "        print(f\"Loaded {len(results['responses'])} raw responses\")    \n",
    "    claims_file = Config.CLAIMS_DIR / f\"claims_{experiment_id}.csv\"\n",
    "    if claims_file.exists():\n",
    "        results['claims'] = pd.read_csv(claims_file)\n",
    "        print(f\"Loaded {len(results['claims'])} responses with claims\")    \n",
    "    metrics_file = Config.ANALYSIS_DIR / f\"stability_metrics_{experiment_id}.csv\"\n",
    "    if metrics_file.exists():\n",
    "        results['metrics'] = pd.read_csv(metrics_file)\n",
    "        print(f\"Loaded {len(results['metrics'])} metric rows\")\n",
    "    return results\n",
    "def list_experiments() -> List[str]:\n",
    "    experiments = set()\n",
    "    for file in Config.RAW_RESPONSES_DIR.glob(\"responses_*.csv\"):\n",
    "        exp_id = file.stem.replace(\"responses_\", \"\")\n",
    "        experiments.add(exp_id)\n",
    "    return sorted(list(experiments))\n",
    "def analyze_concept_stability(df_claims: pd.DataFrame, concept: str, model: str = None):\n",
    "    import ast\n",
    "    def safe_parse_list(x):\n",
    "        if isinstance(x, list):\n",
    "            return x\n",
    "        if isinstance(x, str):\n",
    "            try:\n",
    "                return ast.literal_eval(x)\n",
    "            except:\n",
    "                return []\n",
    "        return []\n",
    "    mask = df_claims['concept'] == concept\n",
    "    if model:\n",
    "        mask = mask & (df_claims['model'] == model)\n",
    "    concept_data = df_claims[mask].copy()\n",
    "    if len(concept_data) == 0:\n",
    "        print(f\"No data found for concept: {concept}\")\n",
    "        return\n",
    "    print(f\"\\n\")\n",
    "    print(f\"Concept Analysis: {concept}\")\n",
    "    if model:\n",
    "        print(f\"Model: {model}\")\n",
    "    print(f\"\\n\")\n",
    "    print(f\"Total responses: {len(concept_data)}\")\n",
    "    print(f\"Unique queries: {concept_data['query'].nunique()}\")\n",
    "    print(f\"Runs per query: {len(concept_data) // concept_data['query'].nunique()}\")    \n",
    "    print(f\"\\nStance Distribution:\")\n",
    "    stance_counts = concept_data['extracted_stance'].value_counts()\n",
    "    for stance, count in stance_counts.items():\n",
    "        print(f\"  {stance}: {count} ({count/len(concept_data)*100:.1f}%)\")    \n",
    "    all_claims = []\n",
    "    for claims_raw in concept_data['extracted_claims']:\n",
    "        claims = safe_parse_list(claims_raw)\n",
    "        all_claims.extend(claims)\n",
    "    if all_claims:\n",
    "        print(f\"\\nTotal claims extracted: {len(all_claims)}\")\n",
    "        print(f\"Unique claims: {len(set(all_claims))}\")\n",
    "        print(f\"Claim repetition: {(1 - len(set(all_claims))/len(all_claims))*100:.1f}%\")\n",
    "    else:\n",
    "        print(\"\\nNo claims extracted\")    \n",
    "    print(f\"\\n\")\n",
    "    print(\"Sample Responses:\")\n",
    "    print(f\"\\n\")\n",
    "    for idx, row in concept_data.head(3).iterrows():\n",
    "        print(f\"\\nQuery: {row['query']}\")\n",
    "        print(f\"Stance: {row['extracted_stance']}\")\n",
    "        claims = safe_parse_list(row['extracted_claims'])\n",
    "        if claims:\n",
    "            print(f\"Claims ({len(claims)}):\")\n",
    "            for claim in claims[:3]:\n",
    "                print(f\"  - {claim}\")\n",
    "            if len(claims) > 3:\n",
    "                print(f\"  ... and {len(claims)-3} more\")\n",
    "        print(\"-\" * 30)\n",
    "def compare_models_on_concept(df_claims: pd.DataFrame, concept: str):\n",
    "    concept_data = df_claims[df_claims['concept'] == concept]\n",
    "    if len(concept_data) == 0:\n",
    "        print(f\"No data found for concept: {concept}\")\n",
    "        return\n",
    "    print(f\"\\n\")\n",
    "    print(f\"Model Comparison: {concept}\")\n",
    "    print(f\"\\n\")\n",
    "    for model in concept_data['model'].unique():\n",
    "        model_data = concept_data[concept_data['model'] == model]\n",
    "        print(f\"\\n{model}:\")\n",
    "        print(f\"  Responses: {len(model_data)}\")\n",
    "        print(f\"  Avg claims/response: {model_data['num_claims'].mean():.2f}\")\n",
    "        print(f\"  Avg response length: {model_data['response'].str.len().mean():.0f} chars\")\n",
    "        stances = model_data['extracted_stance'].value_counts()\n",
    "        print(f\"  Dominant stance: {stances.index[0]} ({stances.iloc[0]} times)\")        \n",
    "        unique_stances = model_data['extracted_stance'].nunique()\n",
    "        print(f\"  Stance consistency: {(1 - (unique_stances-1)/len(model_data))*100:.1f}%\")\n",
    "def export_for_paper(metrics_df: pd.DataFrame, experiment_id: str):\n",
    "    print(\"\\nExporting publication-ready data...\")    \n",
    "    model_summary = metrics_df.groupby('model').agg({\n",
    "        'claim_stability_score': ['mean', 'std'],\n",
    "        'stance_volatility_index': ['mean', 'std'],\n",
    "        'definition_drift_score': ['mean', 'std']\n",
    "    }).round(3)\n",
    "    model_summary.columns = ['_'.join(col) for col in model_summary.columns]\n",
    "    table1_file = Config.ANALYSIS_DIR / f\"table1_model_comparison_{experiment_id}.csv\"\n",
    "    model_summary.to_csv(table1_file)\n",
    "    print(f\"Table 1 saved: {table1_file}\")    \n",
    "    category_summary = metrics_df.groupby(['category', 'model']).agg({\n",
    "        'claim_stability_score': 'mean',\n",
    "        'stance_volatility_index': 'mean',\n",
    "        'definition_drift_score': 'mean'\n",
    "    }).round(3)\n",
    "    table2_file = Config.ANALYSIS_DIR / f\"table2_category_breakdown_{experiment_id}.csv\"\n",
    "    category_summary.to_csv(table2_file)\n",
    "    print(f\"Table 2 saved: {table2_file}\")    \n",
    "    top_stable = metrics_df.nlargest(10, 'claim_stability_score')[\n",
    "        ['concept', 'model', 'claim_stability_score', 'stance_volatility_index']]\n",
    "    bottom_stable = metrics_df.nsmallest(10, 'claim_stability_score')[\n",
    "        ['concept', 'model', 'claim_stability_score', 'stance_volatility_index']]\n",
    "    table3_file = Config.ANALYSIS_DIR / f\"table3_extreme_concepts_{experiment_id}.csv\"\n",
    "    pd.concat([\n",
    "        top_stable.assign(rank='Most Stable'),\n",
    "        bottom_stable.assign(rank='Least Stable')\n",
    "    ]).to_csv(table3_file, index=False)\n",
    "    print(f\"Table 3 saved: {table3_file}\")\n",
    "    print(\"\\nAll tables exported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Starting Complete LLM Stability Experiment\n",
      "\n",
      "This will collect 510 responses total\n",
      "\n",
      "\n",
      "\n",
      "Step 1/5: Collecting model responses...\n",
      "\n",
      "Starting data collection - Experiment ID: 20251217_163437\n",
      "Total responses to collect: 510\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e30256d69a242ff973884e97329041f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Collecting responses:   0%|          | 0/510 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data collection completed.\n",
      "Saved to: experiment_outputs/raw_responses/responses_20251217_163437.csv\n",
      "Success rate: 100.0%\n",
      "\n",
      "Step 2/5: Extracting claims and definitions...\n",
      "\n",
      "Starting claim extraction for 510 responses\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66526e4eef9840b59932906a42435cbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting claims:   0%|          | 0/510 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Claim extraction completed.\n",
      "Saved to: experiment_outputs/extracted_claims/claims_20251217_163437.csv\n",
      "Average claims per response: 6.4\n",
      "Average definitions per response: 1.9\n",
      "\n",
      "Stance distribution:\n",
      "  neutral: 238 (46.7%)\n",
      "  affirmative: 129 (25.3%)\n",
      "  conditional: 115 (22.5%)\n",
      "  negative: 27 (5.3%)\n",
      "  abstention: 1 (0.2%)\n",
      "\n",
      "Step 3/5: Computing stability metrics...\n",
      "\n",
      "Calculating stability metrics (with semantic similarity):\n",
      "Using sentence transformer embeddings\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "394867e4c3ea4e0c9261b7bb687d8306",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing metrics:   0%|          | 0/34 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics calculated for 34 concept-model pairs\n",
      "Saved to: experiment_outputs/analysis/stability_metrics_20251217_163437.csv\n",
      "\n",
      "Metric Comparison (Exact vs Semantic):\n",
      "CSS (exact):    0.028\n",
      "CSS (semantic): 0.453\n",
      "Improvement:    0.425\n",
      "\n",
      "DDS (exact):    0.968\n",
      "DDS (semantic): 0.395\n",
      "Improvement:    0.574\n",
      "\n",
      "Step 4/5: Generating visualizations...\n",
      "\n",
      "\n",
      "Generating visualizations\n",
      "Visualizations saved to: experiment_outputs/plots\n",
      "\n",
      "Step 5/5: Creating summary report...\n",
      "LLM Stability Research - Experimental Summary:\n",
      "\n",
      "Experiment ID: 20251217_163437\n",
      "Date: 2025-12-17 21:23:39\n",
      "\n",
      "\n",
      "Dataset Statistics:\n",
      "Total responses collected: 510\n",
      "Successful responses: 510\n",
      "Success rate: 100.0%\n",
      "Models evaluated: deepseek-r1:7b, llama3:latest\n",
      "Concepts evaluated: 17\n",
      "Categories: 7\n",
      "\n",
      "\n",
      "Extraction Statistics:\n",
      "Average claims per response: 6.42\n",
      "Average definitions per response: 1.85\n",
      "Total unique stances observed: 5\n",
      "\n",
      "\n",
      "Stability Metrics - Overall:\n",
      "Average Claim Stability Score (CSS): 0.453\n",
      "Average Stance Volatility Index (SVI): 0.483\n",
      "Average Definition Drift Score (DDS): 0.395\n",
      "\n",
      "\n",
      "Model Comparison:\n",
      "\n",
      "deepseek-r1:7b:\n",
      "  CSS: 0.442 (±0.130)\n",
      "  SVI: 0.508 (±0.143)\n",
      "  DDS: 0.382 (±0.137)\n",
      "\n",
      "llama3:latest:\n",
      "  CSS: 0.464 (±0.113)\n",
      "  SVI: 0.458 (±0.170)\n",
      "  DDS: 0.407 (±0.074)\n",
      "\n",
      "\n",
      "Top 5 Most Stable Concepts:\n",
      "Inflation (llama3:latest): CSS = 0.657\n",
      "Climate Change (deepseek-r1:7b): CSS = 0.633\n",
      "Studying Late at Night (llama3:latest): CSS = 0.613\n",
      "Remote Work (deepseek-r1:7b): CSS = 0.611\n",
      "Studying Late at Night (deepseek-r1:7b): CSS = 0.610\n",
      "\n",
      "\n",
      "Top 5 Most Volatile Concepts:\n",
      "Inflation (llama3:latest): SVI = 0.786\n",
      "Rules & Exceptions (llama3:latest): SVI = 0.786\n",
      "Inflation (deepseek-r1:7b): SVI = 0.714\n",
      "Social Media (deepseek-r1:7b): SVI = 0.714\n",
      "Artificial Intelligence (deepseek-r1:7b): SVI = 0.643\n",
      "\n",
      "\n",
      "Category-Level Analysis:\n",
      "\n",
      "DEFINITION_STABILITY:\n",
      "  Concepts: 3\n",
      "  Average CSS: 0.450\n",
      "  Average SVI: 0.536\n",
      "  Average DDS: 0.363\n",
      "\n",
      "FACTUAL_EXPLANATORY:\n",
      "  Concepts: 3\n",
      "  Average CSS: 0.536\n",
      "  Average SVI: 0.476\n",
      "  Average DDS: 0.398\n",
      "\n",
      "ADVICE_POLICY:\n",
      "  Concepts: 3\n",
      "  Average CSS: 0.441\n",
      "  Average SVI: 0.429\n",
      "  Average DDS: 0.390\n",
      "\n",
      "TECHNICAL_CONCEPT:\n",
      "  Concepts: 2\n",
      "  Average CSS: 0.482\n",
      "  Average SVI: 0.500\n",
      "  Average DDS: 0.380\n",
      "\n",
      "META_REASONING:\n",
      "  Concepts: 2\n",
      "  Average CSS: 0.319\n",
      "  Average SVI: 0.482\n",
      "  Average DDS: 0.366\n",
      "\n",
      "LOGICAL_EDGE_CASE:\n",
      "  Concepts: 2\n",
      "  Average CSS: 0.359\n",
      "  Average SVI: 0.518\n",
      "  Average DDS: 0.495\n",
      "\n",
      "TEMPORAL_CAUSAL:\n",
      "  Concepts: 2\n",
      "  Average CSS: 0.550\n",
      "  Average SVI: 0.446\n",
      "  Average DDS: 0.390\n",
      "\n",
      "\n",
      "End of Report.\n",
      "\n",
      "Report saved to: experiment_outputs/analysis/summary_report_20251217_163437.txt\n",
      "\n",
      "\n",
      "Experiment Complete.\n",
      "\n",
      "All results saved in: experiment_outputs\n",
      "Experiment ID: 20251217_163437\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = run_complete_experiment() # Main Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical Validation Module\n",
    "def calculate_confidence_intervals(metrics_df: pd.DataFrame, \n",
    "                                  n_bootstrap: int = 1000) -> pd.DataFrame:    \n",
    "    print(f\"\\n\")\n",
    "    print(\"Calculating confidence intervals (bootstrap)\")\n",
    "    print(f\"\\n\")\n",
    "    ci_results = []\n",
    "    for model in metrics_df['model'].unique():\n",
    "        model_data = metrics_df[metrics_df['model'] == model]\n",
    "        for metric in ['claim_stability_score', 'stance_volatility_index', 'definition_drift_score']:\n",
    "            values = model_data[metric].values            \n",
    "            bootstrap_means = []\n",
    "            for _ in range(n_bootstrap):\n",
    "                sample = np.random.choice(values, size=len(values), replace=True)\n",
    "                bootstrap_means.append(np.mean(sample))            \n",
    "            ci_lower = np.percentile(bootstrap_means, 2.5)\n",
    "            ci_upper = np.percentile(bootstrap_means, 97.5)\n",
    "            mean = np.mean(values)\n",
    "            ci_results.append({\n",
    "                'model': model,\n",
    "                'metric': metric,\n",
    "                'mean': mean,\n",
    "                'ci_lower': ci_lower,\n",
    "                'ci_upper': ci_upper,\n",
    "                'ci_width': ci_upper - ci_lower})\n",
    "    ci_df = pd.DataFrame(ci_results)\n",
    "    print(\"\\nConfidence Intervals (95%):\")\n",
    "    for model in ci_df['model'].unique():\n",
    "        print(f\"\\n{model}:\")\n",
    "        model_ci = ci_df[ci_df['model'] == model]\n",
    "        for _, row in model_ci.iterrows():\n",
    "            print(f\"  {row['metric']}: {row['mean']:.3f} [{row['ci_lower']:.3f}, {row['ci_upper']:.3f}]\")\n",
    "    return ci_df\n",
    "def test_model_differences(metrics_df: pd.DataFrame) -> pd.DataFrame:    \n",
    "    print(\"\\n\")\n",
    "    print(\"Testing statistical significance of model differences\")\n",
    "    print(\"\\n\")\n",
    "    models = metrics_df['model'].unique()\n",
    "    if len(models) != 2:\n",
    "        print(\"Need exactly 2 models for comparison\")\n",
    "        return pd.DataFrame()\n",
    "    model1, model2 = models[0], models[1]\n",
    "    m1_data = metrics_df[metrics_df['model'] == model1]\n",
    "    m2_data = metrics_df[metrics_df['model'] == model2]\n",
    "    test_results = []\n",
    "    for metric in ['claim_stability_score', 'stance_volatility_index', 'definition_drift_score']:\n",
    "        values1 = m1_data[metric].values\n",
    "        values2 = m2_data[metric].values        \n",
    "        t_stat, p_value = ttest_ind(values1, values2)        \n",
    "        pooled_std = np.sqrt((np.std(values1)**2 + np.std(values2)**2) / 2)\n",
    "        cohens_d = (np.mean(values1) - np.mean(values2)) / pooled_std if pooled_std > 0 else 0        \n",
    "        if p_value < 0.001:\n",
    "            significance = \"***\"\n",
    "        elif p_value < 0.01:\n",
    "            significance = \"**\"\n",
    "        elif p_value < 0.05:\n",
    "            significance = \"*\"\n",
    "        else:\n",
    "            significance = \"ns\"\n",
    "        test_results.append({\n",
    "            'metric': metric,\n",
    "            'model1': model1,\n",
    "            'mean1': np.mean(values1),\n",
    "            'model2': model2,\n",
    "            'mean2': np.mean(values2),\n",
    "            'difference': np.mean(values1) - np.mean(values2),\n",
    "            't_statistic': t_stat,\n",
    "            'p_value': p_value,\n",
    "            'cohens_d': cohens_d,\n",
    "            'significance': significance})\n",
    "    results_df = pd.DataFrame(test_results)\n",
    "    print(f\"Comparing {model1} vs {model2}:\")\n",
    "    for _, row in results_df.iterrows():\n",
    "        print(f\"\\n{row['metric']}:\")\n",
    "        print(f\"  {model1}: {row['mean1']:.3f}\")\n",
    "        print(f\"  {model2}: {row['mean2']:.3f}\")\n",
    "        print(f\"  Difference: {row['difference']:.3f}\")\n",
    "        print(f\"  p-value: {row['p_value']:.4f} {row['significance']}\")\n",
    "        print(f\"  Effect size (Cohen's d): {row['cohens_d']:.3f}\")\n",
    "    print(f\"\\n\")\n",
    "    print(\"*** p<0.001, ** p<0.01, * p<0.05, ns = not significant\")\n",
    "    return results_df\n",
    "def analyze_category_differences(metrics_df: pd.DataFrame) -> pd.DataFrame:    \n",
    "    print(f\"\\n\")\n",
    "    print(\"Category-level stability analysis\")\n",
    "    print(f\"\\n\")\n",
    "    category_results = []\n",
    "    for category in metrics_df['category'].unique():\n",
    "        cat_data = metrics_df[metrics_df['category'] == category]\n",
    "        category_results.append({\n",
    "            'category': category,\n",
    "            'num_concepts': cat_data['concept'].nunique(),\n",
    "            'css_mean': cat_data['claim_stability_score'].mean(),\n",
    "            'css_std': cat_data['claim_stability_score'].std(),\n",
    "            'svi_mean': cat_data['stance_volatility_index'].mean(),\n",
    "            'svi_std': cat_data['stance_volatility_index'].std(),\n",
    "            'dds_mean': cat_data['definition_drift_score'].mean(),\n",
    "            'dds_std': cat_data['definition_drift_score'].std(),})\n",
    "    cat_df = pd.DataFrame(category_results)\n",
    "    cat_df = cat_df.sort_values('css_mean', ascending=False)\n",
    "    print(\"Categories ranked by claim stability (CSS):\")\n",
    "    print(cat_df[['category', 'css_mean', 'svi_mean', 'dds_mean']].to_string(index=False))\n",
    "    return cat_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qualitative Analysis Module\n",
    "def extract_paper_examples(df_claims, metrics_df, experiment_id, num_examples=10):    \n",
    "    import ast\n",
    "    def safe_parse_list(x):\n",
    "        if isinstance(x, list):\n",
    "            return x\n",
    "        if isinstance(x, str):\n",
    "            try:\n",
    "                return ast.literal_eval(x)\n",
    "            except:\n",
    "                return []\n",
    "        return []\n",
    "    def safe_parse_dict(x):\n",
    "        if isinstance(x, dict):\n",
    "            return x\n",
    "        if isinstance(x, str):\n",
    "            try:\n",
    "                return ast.literal_eval(x)\n",
    "            except:\n",
    "                return {}\n",
    "        return {}\n",
    "    print(f\"\\n\")\n",
    "    print(\"Extracting Qualitative Examples\")\n",
    "    print(f\"\\n\")\n",
    "    examples = []    \n",
    "    print(\"Example Type 1: Stance Volatility\\n\")\n",
    "    volatile = metrics_df.nlargest(3, 'stance_volatility_index')\n",
    "    for _, row in volatile.iterrows():\n",
    "        concept = row['concept']\n",
    "        model = row['model']\n",
    "        concept_data = df_claims[\n",
    "            (df_claims['concept'] == concept) & \n",
    "            (df_claims['model'] == model)\n",
    "        ].head(3)\n",
    "        print(f\"--- {concept} ({model}) - SVI: {row['stance_volatility_index']:.3f} ---\\n\")\n",
    "        for idx, resp_row in concept_data.iterrows():\n",
    "            print(f\"Query: {resp_row['query']}\")\n",
    "            print(f\"Stance: {resp_row['extracted_stance']}\")            \n",
    "            claims = safe_parse_list(resp_row['extracted_claims'])\n",
    "            if claims:\n",
    "                print(f\"Sample claims:\")\n",
    "                for claim in claims[:2]:\n",
    "                    print(f\"  • {claim}\")\n",
    "            print()\n",
    "        examples.append({\n",
    "            'type': 'stance_volatility',\n",
    "            'concept': concept,\n",
    "            'model': model,\n",
    "            'svi': row['stance_volatility_index']})    \n",
    "    print(\"\\nExample Type 2: Definition Drift\\n\")\n",
    "    drifty = metrics_df.nlargest(3, 'definition_drift_score')\n",
    "    for _, row in drifty.iterrows():\n",
    "        concept = row['concept']\n",
    "        model = row['model']\n",
    "        concept_data = df_claims[\n",
    "            (df_claims['concept'] == concept) & \n",
    "            (df_claims['model'] == model)]\n",
    "        print(f\"--- {concept} ({model}) - DDS: {row['definition_drift_score']:.3f} ---\")\n",
    "        print(\"Definitions provided:\\n\")\n",
    "        found_definitions = False\n",
    "        for i, defs_raw in enumerate(concept_data['extracted_definitions'].head(3)):\n",
    "            defs = safe_parse_dict(defs_raw)\n",
    "            if defs:\n",
    "                found_definitions = True\n",
    "                print(f\"Response {i+1}:\")\n",
    "                for term, definition in defs.items():\n",
    "                    print(f\"  {term}: {definition}\")\n",
    "                print()\n",
    "        if not found_definitions:\n",
    "            print(\"No explicit definitions found in responses.\\n\")\n",
    "        examples.append({\n",
    "            'type': 'definition_drift',\n",
    "            'concept': concept,\n",
    "            'model': model,\n",
    "            'dds': row['definition_drift_score']})\n",
    "    print(\"\\nExample Type 3: Inflation Paradox\\n\")\n",
    "    inflation_data = df_claims[\n",
    "        (df_claims['concept'] == 'Inflation') & \n",
    "        (df_claims['model'] == 'deepseek-r1:7b')\n",
    "    ].head(5)\n",
    "    if len(inflation_data) > 0:\n",
    "        print(\"--- Inflation (DeepSeek) ---\")\n",
    "        print(\"Stable factual claims + volatile stance\\n\")\n",
    "        for idx, row in inflation_data.iterrows():\n",
    "            print(f\"Query: {row['query']}\")\n",
    "            print(f\"Stance: {row['extracted_stance']}\")\n",
    "            claims = safe_parse_list(row['extracted_claims'])\n",
    "            if claims:\n",
    "                print(\"Claims:\")\n",
    "                for claim in claims[:3]:\n",
    "                    print(f\"  • {claim}\")\n",
    "            print()\n",
    "        examples.append({\n",
    "            'type': 'inflation_paradox',\n",
    "            'concept': 'Inflation',\n",
    "            'model': 'deepseek-r1:7b'})\n",
    "    examples_file = Config.ANALYSIS_DIR / f\"paper_examples_{experiment_id}.json\"\n",
    "    with open(examples_file, 'w') as f:\n",
    "        json.dump(examples, f, indent=2)\n",
    "    print(f\"\\nExamples saved: {examples_file}\")\n",
    "    print(f\"Total examples: {len(examples)}\\n\")\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional Visualizations\n",
    "def create_additional_visualizations(metrics_df, experiment_id):    \n",
    "    print(f\"\\n\")\n",
    "    print(\"Creating additional visualizations\")\n",
    "    print(f\"\\n\")    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for model in metrics_df['model'].unique():\n",
    "        model_data = metrics_df[metrics_df['model'] == model]\n",
    "        plt.scatter(model_data['claim_stability_score'], \n",
    "                    model_data['stance_volatility_index'],\n",
    "                    label=model, s=100, alpha=0.7)        \n",
    "        for _, row in model_data.iterrows():\n",
    "            if row['stance_volatility_index'] > 0.6 or row['claim_stability_score'] > 0.55:\n",
    "                plt.annotate(row['concept'], \n",
    "                            (row['claim_stability_score'], row['stance_volatility_index']),\n",
    "                            fontsize=8, alpha=0.7)\n",
    "    plt.xlabel('Claim Stability Score (CSS)', fontsize=12)\n",
    "    plt.ylabel('Stance Volatility Index (SVI)', fontsize=12)\n",
    "    plt.title('Claim Stability vs Stance Volatility', fontsize=14, weight='bold')\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(Config.PLOTS_DIR / f\"css_vs_svi_scatter_{experiment_id}.png\", dpi=300)\n",
    "    plt.close()\n",
    "    print(\"Created CSS vs SVI scatter plot\")    \n",
    "    pivot_css = metrics_df.pivot(index='concept', columns='model', values='claim_stability_score')\n",
    "    pivot_svi = metrics_df.pivot(index='concept', columns='model', values='stance_volatility_index')\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 10))\n",
    "    sns.heatmap(pivot_css, annot=True, fmt='.2f', cmap='YlGn', ax=axes[0], \n",
    "                cbar_kws={'label': 'CSS'}, vmin=0, vmax=0.7)\n",
    "    axes[0].set_title('Claim Stability Score by Concept', fontsize=12, weight='bold')\n",
    "    sns.heatmap(pivot_svi, annot=True, fmt='.2f', cmap='YlOrRd', ax=axes[1], \n",
    "                cbar_kws={'label': 'SVI'}, vmin=0, vmax=0.8)\n",
    "    axes[1].set_title('Stance Volatility Index by Concept', fontsize=12, weight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(Config.PLOTS_DIR / f\"heatmap_metrics_{experiment_id}.png\", dpi=300)\n",
    "    plt.close()\n",
    "    print(\"Created metrics heatmap\")    \n",
    "    cat_ranks = metrics_df.groupby('category').agg({\n",
    "        'claim_stability_score': 'mean',\n",
    "        'stance_volatility_index': 'mean',\n",
    "        'definition_drift_score': 'mean'\n",
    "    }).sort_values('claim_stability_score', ascending=False)\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    x = range(len(cat_ranks))\n",
    "    width = 0.25\n",
    "    ax.bar([i - width for i in x], cat_ranks['claim_stability_score'], \n",
    "           width, label='CSS (↑ better)', color='steelblue')\n",
    "    ax.bar([i for i in x], 1 - cat_ranks['stance_volatility_index'], \n",
    "           width, label='Stance Stability (↑ better)', color='coral')\n",
    "    ax.bar([i + width for i in x], 1 - cat_ranks['definition_drift_score'], \n",
    "           width, label='Definition Stability (↑ better)', color='mediumseagreen')\n",
    "    ax.set_ylabel('Score', fontsize=12)\n",
    "    ax.set_title('Category Stability Rankings', fontsize=14, weight='bold')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(cat_ranks.index, rotation=45, ha='right')\n",
    "    ax.legend()\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(Config.PLOTS_DIR / f\"category_rankings_{experiment_id}.png\", dpi=300)\n",
    "    plt.close()\n",
    "    print(\"Created category rankings plot\")\n",
    "    print(\"\\nAll additional visualizations created.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Comprehensive Summary\n",
    "def generate_final_summary(df_claims, metrics_df, ci_df, test_results, experiment_id):    \n",
    "    summary = []    \n",
    "    summary.append(\"Comprehensive Research Summary:\")\n",
    "    summary.append(\"LLM Stability and Internal Consistency Study -\")\n",
    "    summary.append(f\"\\nExperiment ID: {experiment_id}\")\n",
    "    summary.append(f\"Analysis Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    summary.append(\"\\n\")\n",
    "    summary.append(\"Key Findings:\")\n",
    "    summary.append(\"\\n1. Overall Stability\")\n",
    "    summary.append(f\"   • Claim Stability Score (CSS): {metrics_df['claim_stability_score'].mean():.3f}\")\n",
    "    summary.append(f\"     → LLMs maintain ~{metrics_df['claim_stability_score'].mean()*100:.1f}% semantic overlap\")\n",
    "    summary.append(f\"   • Stance Volatility Index (SVI): {metrics_df['stance_volatility_index'].mean():.3f}\")\n",
    "    summary.append(f\"     → Models flip stance in ~{metrics_df['stance_volatility_index'].mean()*100:.1f}% of cases\")\n",
    "    summary.append(f\"   • Definition Drift Score (DDS): {metrics_df['definition_drift_score'].mean():.3f}\")\n",
    "    summary.append(f\"     → Definitions drift by ~{metrics_df['definition_drift_score'].mean()*100:.1f}%\")    \n",
    "    summary.append(\"\\n2. Model Comparison\")\n",
    "    for model in metrics_df['model'].unique():\n",
    "        model_data = metrics_df[metrics_df['model'] == model]\n",
    "        summary.append(f\"\\n   {model}:\")\n",
    "        summary.append(f\"   • CSS: {model_data['claim_stability_score'].mean():.3f} (±{model_data['claim_stability_score'].std():.3f})\")\n",
    "        summary.append(f\"   • SVI: {model_data['stance_volatility_index'].mean():.3f} (±{model_data['stance_volatility_index'].std():.3f})\")\n",
    "        summary.append(f\"   • DDS: {model_data['definition_drift_score'].mean():.3f} (±{model_data['definition_drift_score'].std():.3f})\")    \n",
    "    summary.append(\"\\n3. Statistical Significance\")\n",
    "    for _, row in test_results.iterrows():\n",
    "        summary.append(f\"\\n   {row['metric']}:\")\n",
    "        summary.append(f\"   • Difference: {row['difference']:.3f}\")\n",
    "        summary.append(f\"   • p-value: {row['p_value']:.4f} {row['significance']}\")\n",
    "        summary.append(f\"   • Effect size: {row['cohens_d']:.3f}\")    \n",
    "    summary.append(\"\\n4. Category Insights\")\n",
    "    cat_summary = metrics_df.groupby('category').agg({\n",
    "        'claim_stability_score': 'mean',\n",
    "        'stance_volatility_index': 'mean'\n",
    "    }).sort_values('claim_stability_score', ascending=False)\n",
    "    summary.append(f\"\\n   Most Stable: {cat_summary.index[0]} (CSS: {cat_summary.iloc[0]['claim_stability_score']:.3f})\")\n",
    "    summary.append(f\"   Least Stable: {cat_summary.index[-1]} (CSS: {cat_summary.iloc[-1]['claim_stability_score']:.3f})\")\n",
    "    summary.append(f\"   Stability Range: {(cat_summary.iloc[0]['claim_stability_score'] - cat_summary.iloc[-1]['claim_stability_score']):.3f}\")    \n",
    "    summary.append(\"\\n5. Inflation Paradox\")\n",
    "    inflation = metrics_df[metrics_df['concept'] == 'Inflation']\n",
    "    summary.append(f\"   • High factual stability: CSS = {inflation['claim_stability_score'].mean():.3f}\")\n",
    "    summary.append(f\"   • High stance volatility: SVI = {inflation['stance_volatility_index'].mean():.3f}\")\n",
    "    summary.append(\"\\n\")\n",
    "    summary.append(\"Outputs Generated.\")\n",
    "    summary.append(\"\\nData collection: 510/510 responses\")\n",
    "    summary.append(\"Semantic metrics calculated.\")\n",
    "    summary.append(\"Statistical tests performed.\")\n",
    "    summary.append(\"Qualitative examples extracted.\")\n",
    "    summary.append(\"Publication-ready tables exported.\")\n",
    "    summary.append(\"Comprehensive visualizations created.\")\n",
    "    summary.append(\"\\n\")\n",
    "    summary.append(\"End of Summary.\")\n",
    "    summary_text = \"\\n\".join(summary)\n",
    "    summary_file = Config.ANALYSIS_DIR / f\"final_summary_{experiment_id}.txt\"\n",
    "    with open(summary_file, 'w') as f:\n",
    "        f.write(summary_text)\n",
    "    print(summary_text)\n",
    "    print(f\"\\nFinal summary saved to: {summary_file}\\n\")\n",
    "    return summary_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete Post-Experiment Analysis\n",
    "def run_complete_analysis(experiment_id=None):    \n",
    "    import ast\n",
    "    def safe_parse_list(x):\n",
    "        if isinstance(x, list):\n",
    "            return x\n",
    "        if isinstance(x, str):\n",
    "            try:\n",
    "                return ast.literal_eval(x)\n",
    "            except:\n",
    "                return []\n",
    "        return []\n",
    "    def safe_parse_dict(x):\n",
    "        if isinstance(x, dict):\n",
    "            return x\n",
    "        if isinstance(x, str):\n",
    "            try:\n",
    "                return ast.literal_eval(x)\n",
    "            except:\n",
    "                return {}\n",
    "        return {}\n",
    "    print(\"Complete Post-Experiment Analysis\")    \n",
    "    experiments = list_experiments()\n",
    "    if experiment_id is None:\n",
    "        experiment_id = experiments[-1]\n",
    "    print(f\"\\nLoading experiment: {experiment_id}\")\n",
    "    data = load_experiment_results(experiment_id)\n",
    "    df_claims = data['claims']\n",
    "    metrics_df = data['metrics']    \n",
    "    print(\"Parsing data structures...\")\n",
    "    df_claims['extracted_claims'] = df_claims['extracted_claims'].apply(safe_parse_list)\n",
    "    df_claims['extracted_definitions'] = df_claims['extracted_definitions'].apply(safe_parse_dict)\n",
    "    print(\"Data structures parsed\\n\")    \n",
    "    print(\"\\nStatistical Analysis\")\n",
    "    ci_df = calculate_confidence_intervals(metrics_df, n_bootstrap=1000)\n",
    "    test_results = test_model_differences(metrics_df)\n",
    "    category_analysis = analyze_category_differences(metrics_df)    \n",
    "    ci_df.to_csv(Config.ANALYSIS_DIR / f\"confidence_intervals_{experiment_id}.csv\", index=False)\n",
    "    test_results.to_csv(Config.ANALYSIS_DIR / f\"significance_tests_{experiment_id}.csv\", index=False)\n",
    "    category_analysis.to_csv(Config.ANALYSIS_DIR / f\"category_analysis_{experiment_id}.csv\", index=False)\n",
    "    print(\"\\nStatistical results saved\")\n",
    "    print(\"\\nConcept Deep Dives\")\n",
    "    print(\"\\n1. Inflation Paradox:\")\n",
    "    analyze_concept_stability(df_claims, \"Inflation\", model=\"deepseek-r1:7b\")\n",
    "    compare_models_on_concept(df_claims, \"Inflation\")\n",
    "    print(\"\\n2. Most Volatile:\")\n",
    "    most_volatile = metrics_df.nlargest(1, 'stance_volatility_index').iloc[0]\n",
    "    analyze_concept_stability(df_claims, most_volatile['concept'], model=most_volatile['model'])\n",
    "    print(\"\\n3. Most Stable:\")\n",
    "    most_stable = metrics_df.nlargest(1, 'claim_stability_score').iloc[0]\n",
    "    analyze_concept_stability(df_claims, most_stable['concept'], model=most_stable['model'])    \n",
    "    examples = extract_paper_examples(df_claims, metrics_df, experiment_id)    \n",
    "    print(\"\\nExporting Publication Tables\")\n",
    "    export_for_paper(metrics_df, experiment_id)\n",
    "    print(\"\\nCreating Visualizations\")\n",
    "    create_additional_visualizations(metrics_df, experiment_id)\n",
    "    print(\"\\nGenerating Final Summary\")\n",
    "    summary = generate_final_summary(df_claims, metrics_df, ci_df, test_results, experiment_id)    \n",
    "    print(\"\\nComplete Analysis Finished.\")    \n",
    "    return {\n",
    "        'df_claims': df_claims, 'metrics_df': metrics_df, 'ci_df': ci_df, 'test_results': test_results, \n",
    "        'category_analysis': category_analysis, 'examples': examples, 'experiment_id': experiment_id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete Post-Experiment Analysis\n",
      "\n",
      "Loading experiment: 20251217_163437\n",
      "Loading experiment: 20251217_163437\n",
      "Loaded 510 raw responses\n",
      "Loaded 510 responses with claims\n",
      "Loaded 34 metric rows\n",
      "Parsing data structures...\n",
      "Data structures parsed\n",
      "\n",
      "\n",
      "Statistical Analysis\n",
      "\n",
      "\n",
      "Calculating confidence intervals (bootstrap)\n",
      "\n",
      "\n",
      "\n",
      "Confidence Intervals (95%):\n",
      "\n",
      "deepseek-r1:7b:\n",
      "  claim_stability_score: 0.442 [0.382, 0.500]\n",
      "  stance_volatility_index: 0.508 [0.445, 0.576]\n",
      "  definition_drift_score: 0.382 [0.311, 0.445]\n",
      "\n",
      "llama3:latest:\n",
      "  claim_stability_score: 0.464 [0.410, 0.515]\n",
      "  stance_volatility_index: 0.458 [0.382, 0.542]\n",
      "  definition_drift_score: 0.407 [0.374, 0.443]\n",
      "\n",
      "\n",
      "Testing statistical significance of model differences\n",
      "\n",
      "\n",
      "Comparing deepseek-r1:7b vs llama3:latest:\n",
      "\n",
      "claim_stability_score:\n",
      "  deepseek-r1:7b: 0.442\n",
      "  llama3:latest: 0.464\n",
      "  Difference: -0.023\n",
      "  p-value: 0.5923 ns\n",
      "  Effect size (Cohen's d): -0.191\n",
      "\n",
      "stance_volatility_index:\n",
      "  deepseek-r1:7b: 0.508\n",
      "  llama3:latest: 0.458\n",
      "  Difference: 0.050\n",
      "  p-value: 0.3550 ns\n",
      "  Effect size (Cohen's d): 0.332\n",
      "\n",
      "definition_drift_score:\n",
      "  deepseek-r1:7b: 0.382\n",
      "  llama3:latest: 0.407\n",
      "  Difference: -0.025\n",
      "  p-value: 0.5125 ns\n",
      "  Effect size (Cohen's d): -0.234\n",
      "\n",
      "\n",
      "*** p<0.001, ** p<0.01, * p<0.05, ns = not significant\n",
      "\n",
      "\n",
      "Category-level stability analysis\n",
      "\n",
      "\n",
      "Categories ranked by claim stability (CSS):\n",
      "            category  css_mean  svi_mean  dds_mean\n",
      "     TEMPORAL_CAUSAL  0.550233  0.446429  0.389906\n",
      " FACTUAL_EXPLANATORY  0.535595  0.476190  0.397822\n",
      "   TECHNICAL_CONCEPT  0.481644  0.500000  0.380025\n",
      "DEFINITION_STABILITY  0.450427  0.535714  0.362724\n",
      "       ADVICE_POLICY  0.441261  0.428571  0.389735\n",
      "   LOGICAL_EDGE_CASE  0.358944  0.517857  0.495339\n",
      "      META_REASONING  0.318737  0.482143  0.366488\n",
      "\n",
      "Statistical results saved\n",
      "\n",
      "Concept Deep Dives\n",
      "\n",
      "1. Inflation Paradox:\n",
      "\n",
      "\n",
      "Concept Analysis: Inflation\n",
      "Model: deepseek-r1:7b\n",
      "\n",
      "\n",
      "Total responses: 15\n",
      "Unique queries: 5\n",
      "Runs per query: 3\n",
      "\n",
      "Stance Distribution:\n",
      "  neutral: 8 (53.3%)\n",
      "  conditional: 3 (20.0%)\n",
      "  affirmative: 2 (13.3%)\n",
      "  negative: 1 (6.7%)\n",
      "  abstention: 1 (6.7%)\n",
      "\n",
      "Total claims extracted: 72\n",
      "Unique claims: 69\n",
      "Claim repetition: 4.2%\n",
      "\n",
      "\n",
      "Sample Responses:\n",
      "\n",
      "\n",
      "\n",
      "Query: Is inflation always harmful to an economy?\n",
      "Stance: conditional\n",
      "Claims (5):\n",
      "  - Inflation is a complex economic phenomenon.\n",
      "  - Moderate inflation is generally seen as beneficial because it encourages spending, investment, and production.\n",
      "  - Excessive or spiraling inflation can distort the economy, reduce purchasing power, and harm long-term economic stability.\n",
      "  ... and 2 more\n",
      "------------------------------\n",
      "\n",
      "Query: Is inflation always harmful to an economy?\n",
      "Stance: neutral\n",
      "Claims (5):\n",
      "  - Inflation can be a challenge for an economy.\n",
      "  - High inflation can lead to reduced purchasing power.\n",
      "  - High inflation can also lead to increased interest rates.\n",
      "  ... and 2 more\n",
      "------------------------------\n",
      "\n",
      "Query: Is inflation always harmful to an economy?\n",
      "Stance: negative\n",
      "Claims (5):\n",
      "  - Inflation can have negative effects on an economy.\n",
      "  - High inflation rates can erode purchasing power.\n",
      "  - Low inflation rates can lead to deflation, which can also harm an economy.\n",
      "  ... and 2 more\n",
      "------------------------------\n",
      "\n",
      "\n",
      "Model Comparison: Inflation\n",
      "\n",
      "\n",
      "\n",
      "deepseek-r1:7b:\n",
      "  Responses: 15\n",
      "  Avg claims/response: 4.80\n",
      "  Avg response length: 596 chars\n",
      "  Dominant stance: neutral (8 times)\n",
      "  Stance consistency: 73.3%\n",
      "\n",
      "llama3:latest:\n",
      "  Responses: 15\n",
      "  Avg claims/response: 8.93\n",
      "  Avg response length: 2375 chars\n",
      "  Dominant stance: conditional (8 times)\n",
      "  Stance consistency: 93.3%\n",
      "\n",
      "2. Most Volatile:\n",
      "\n",
      "\n",
      "Concept Analysis: Inflation\n",
      "Model: llama3:latest\n",
      "\n",
      "\n",
      "Total responses: 15\n",
      "Unique queries: 5\n",
      "Runs per query: 3\n",
      "\n",
      "Stance Distribution:\n",
      "  conditional: 8 (53.3%)\n",
      "  neutral: 7 (46.7%)\n",
      "\n",
      "Total claims extracted: 134\n",
      "Unique claims: 131\n",
      "Claim repetition: 2.2%\n",
      "\n",
      "\n",
      "Sample Responses:\n",
      "\n",
      "\n",
      "\n",
      "Query: Is inflation always harmful to an economy?\n",
      "Stance: conditional\n",
      "Claims (8):\n",
      "  - Inflation can have some positive effects on an economy.\n",
      "  - A small amount of inflation can encourage people to spend their money sooner rather than later, which can boost economic activity.\n",
      "  - Moderate inflation can lead to higher wages and job creation as businesses try to keep up with rising costs.\n",
      "  ... and 5 more\n",
      "------------------------------\n",
      "\n",
      "Query: Is inflation always harmful to an economy?\n",
      "Stance: conditional\n",
      "Claims (11):\n",
      "  - Inflation is not always harmful to an economy.\n",
      "  - A moderate level of inflation can have positive effects on the economy.\n",
      "  - High or hyperinflation (above 50%) can be detrimental.\n",
      "  ... and 8 more\n",
      "------------------------------\n",
      "\n",
      "Query: Is inflation always harmful to an economy?\n",
      "Stance: neutral\n",
      "Claims (11):\n",
      "  - Inflation is not always harmful to an economy.\n",
      "  - A moderate level of inflation can have some positive effects.\n",
      "  - High or persistent inflation can be detrimental to an economy.\n",
      "  ... and 8 more\n",
      "------------------------------\n",
      "\n",
      "3. Most Stable:\n",
      "\n",
      "\n",
      "Concept Analysis: Inflation\n",
      "Model: llama3:latest\n",
      "\n",
      "\n",
      "Total responses: 15\n",
      "Unique queries: 5\n",
      "Runs per query: 3\n",
      "\n",
      "Stance Distribution:\n",
      "  conditional: 8 (53.3%)\n",
      "  neutral: 7 (46.7%)\n",
      "\n",
      "Total claims extracted: 134\n",
      "Unique claims: 131\n",
      "Claim repetition: 2.2%\n",
      "\n",
      "\n",
      "Sample Responses:\n",
      "\n",
      "\n",
      "\n",
      "Query: Is inflation always harmful to an economy?\n",
      "Stance: conditional\n",
      "Claims (8):\n",
      "  - Inflation can have some positive effects on an economy.\n",
      "  - A small amount of inflation can encourage people to spend their money sooner rather than later, which can boost economic activity.\n",
      "  - Moderate inflation can lead to higher wages and job creation as businesses try to keep up with rising costs.\n",
      "  ... and 5 more\n",
      "------------------------------\n",
      "\n",
      "Query: Is inflation always harmful to an economy?\n",
      "Stance: conditional\n",
      "Claims (11):\n",
      "  - Inflation is not always harmful to an economy.\n",
      "  - A moderate level of inflation can have positive effects on the economy.\n",
      "  - High or hyperinflation (above 50%) can be detrimental.\n",
      "  ... and 8 more\n",
      "------------------------------\n",
      "\n",
      "Query: Is inflation always harmful to an economy?\n",
      "Stance: neutral\n",
      "Claims (11):\n",
      "  - Inflation is not always harmful to an economy.\n",
      "  - A moderate level of inflation can have some positive effects.\n",
      "  - High or persistent inflation can be detrimental to an economy.\n",
      "  ... and 8 more\n",
      "------------------------------\n",
      "\n",
      "\n",
      "Extracting Qualitative Examples\n",
      "\n",
      "\n",
      "Example Type 1: Stance Volatility\n",
      "\n",
      "--- Inflation (llama3:latest) - SVI: 0.786 ---\n",
      "\n",
      "Query: Is inflation always harmful to an economy?\n",
      "Stance: conditional\n",
      "Sample claims:\n",
      "  • Inflation can have some positive effects on an economy.\n",
      "  • A small amount of inflation can encourage people to spend their money sooner rather than later, which can boost economic activity.\n",
      "\n",
      "Query: Is inflation always harmful to an economy?\n",
      "Stance: conditional\n",
      "Sample claims:\n",
      "  • Inflation is not always harmful to an economy.\n",
      "  • A moderate level of inflation can have positive effects on the economy.\n",
      "\n",
      "Query: Is inflation always harmful to an economy?\n",
      "Stance: neutral\n",
      "Sample claims:\n",
      "  • Inflation is not always harmful to an economy.\n",
      "  • A moderate level of inflation can have some positive effects.\n",
      "\n",
      "--- Rules & Exceptions (llama3:latest) - SVI: 0.786 ---\n",
      "\n",
      "Query: Should rules always be followed?\n",
      "Stance: conditional\n",
      "Sample claims:\n",
      "  • Following rules is important for maintaining order.\n",
      "  • Rules provide structure and guidance in various aspects of life.\n",
      "\n",
      "Query: Should rules always be followed?\n",
      "Stance: conditional\n",
      "Sample claims:\n",
      "  • In most cases, following rules is important and necessary to maintain order, ensure fairness, and protect individuals.\n",
      "  • Rules help establish a sense of structure, predictability, and consistency in various aspects of life.\n",
      "\n",
      "Query: Should rules always be followed?\n",
      "Stance: conditional\n",
      "Sample claims:\n",
      "  • Rules can provide structure, guidance, and safety.\n",
      "  • In emergency situations, strict adherence to rules might hinder quick decision-making or action.\n",
      "\n",
      "--- Inflation (deepseek-r1:7b) - SVI: 0.714 ---\n",
      "\n",
      "Query: Is inflation always harmful to an economy?\n",
      "Stance: conditional\n",
      "Sample claims:\n",
      "  • Inflation is a complex economic phenomenon.\n",
      "  • Moderate inflation is generally seen as beneficial because it encourages spending, investment, and production.\n",
      "\n",
      "Query: Is inflation always harmful to an economy?\n",
      "Stance: neutral\n",
      "Sample claims:\n",
      "  • Inflation can be a challenge for an economy.\n",
      "  • High inflation can lead to reduced purchasing power.\n",
      "\n",
      "Query: Is inflation always harmful to an economy?\n",
      "Stance: negative\n",
      "Sample claims:\n",
      "  • Inflation can have negative effects on an economy.\n",
      "  • High inflation rates can erode purchasing power.\n",
      "\n",
      "\n",
      "Example Type 2: Definition Drift\n",
      "\n",
      "--- Uncertainty (deepseek-r1:7b) - DDS: 0.592 ---\n",
      "Definitions provided:\n",
      "\n",
      "Response 2:\n",
      "  Uncertainty: an inherent part of life and knowledge, stemming from the limits of human understanding, measurement, and prediction.\n",
      "\n",
      "Response 3:\n",
      "  None explicitly provided. However, \"uncertainty\": implied to refer to the concept of lack of knowledge or predictability.\n",
      "\n",
      "--- Remote Work (llama3:latest) - DDS: 0.544 ---\n",
      "Definitions provided:\n",
      "\n",
      "Response 1:\n",
      "  None explicitly provided. However, it can be inferred that \"productivity\" refers to the effectiveness or efficiency with which work: accomplished.\n",
      "\n",
      "Response 2:\n",
      "  Office: Not explicitly defined, but implied as a traditional work environment.\n",
      "  Remote work: Not explicitly defined, but implied as working outside an office setting.\n",
      "\n",
      "Response 3:\n",
      "  None explicit definitions provided. However, implicit definitions include: \"remote work\" as working outside of an office setting; \"productivity\" as the measure of output or efficiency in a given task or period; and \"office work\" as traditional employment in a shared workspace.\n",
      "\n",
      "--- Lying (llama3:latest) - DDS: 0.535 ---\n",
      "Definitions provided:\n",
      "\n",
      "Response 1:\n",
      "  White lie: not explicitly defined, but implied to refer to a minor or harmless falsehood told to avoid hurting someone's feelings.\n",
      "  Self-defense: not explicitly defined, but implied to refer to actions taken to protect oneself from harm.\n",
      "\n",
      "Response 2:\n",
      "  Honesty: a fundamental virtue\n",
      "  White lie: a small, harmless fib intended to avoid hurting someone's feelings or spare them unnecessary worry\n",
      "  Lie of omission: withholding information rather than telling an outright lie\n",
      "\n",
      "Response 3:\n",
      "  Intent: whether the lie was intentional or accidental\n",
      "  Context: the situation in which the lie occurs\n",
      "  Consequences: the impact of the lie on others\n",
      "\n",
      "\n",
      "Example Type 3: Inflation Paradox\n",
      "\n",
      "--- Inflation (DeepSeek) ---\n",
      "Stable factual claims + volatile stance\n",
      "\n",
      "Query: Is inflation always harmful to an economy?\n",
      "Stance: conditional\n",
      "Claims:\n",
      "  • Inflation is a complex economic phenomenon.\n",
      "  • Moderate inflation is generally seen as beneficial because it encourages spending, investment, and production.\n",
      "  • Excessive or spiraling inflation can distort the economy, reduce purchasing power, and harm long-term economic stability.\n",
      "\n",
      "Query: Is inflation always harmful to an economy?\n",
      "Stance: neutral\n",
      "Claims:\n",
      "  • Inflation can be a challenge for an economy.\n",
      "  • High inflation can lead to reduced purchasing power.\n",
      "  • High inflation can also lead to increased interest rates.\n",
      "\n",
      "Query: Is inflation always harmful to an economy?\n",
      "Stance: negative\n",
      "Claims:\n",
      "  • Inflation can have negative effects on an economy.\n",
      "  • High inflation rates can erode purchasing power.\n",
      "  • Low inflation rates can lead to deflation, which can also harm an economy.\n",
      "\n",
      "Query: Does inflation have only negative effects?\n",
      "Stance: neutral\n",
      "Claims:\n",
      "  • Inflation can have both positive and negative effects.\n",
      "  • The impact of inflation on an economy depends on various factors such as interest rates, employment, and GDP growth.\n",
      "  • A moderate level of inflation can stimulate economic activity by increasing consumer spending and investment.\n",
      "\n",
      "Query: Does inflation have only negative effects?\n",
      "Stance: neutral\n",
      "Claims:\n",
      "  • Inflation can have both positive and negative impacts.\n",
      "  • Moderate inflation can stimulate economic growth by encouraging increased spending and investment.\n",
      "  • Excessive inflation erodes purchasing power, distorts market expectations, and disrupts economic planning.\n",
      "\n",
      "\n",
      "Examples saved: experiment_outputs/analysis/paper_examples_20251217_163437.json\n",
      "Total examples: 7\n",
      "\n",
      "\n",
      "Exporting Publication Tables\n",
      "\n",
      "Exporting publication-ready data...\n",
      "Table 1 saved: experiment_outputs/analysis/table1_model_comparison_20251217_163437.csv\n",
      "Table 2 saved: experiment_outputs/analysis/table2_category_breakdown_20251217_163437.csv\n",
      "Table 3 saved: experiment_outputs/analysis/table3_extreme_concepts_20251217_163437.csv\n",
      "\n",
      "All tables exported.\n",
      "\n",
      "Creating Visualizations\n",
      "\n",
      "\n",
      "Creating additional visualizations\n",
      "\n",
      "\n",
      "Created CSS vs SVI scatter plot\n",
      "Created metrics heatmap\n",
      "Created category rankings plot\n",
      "\n",
      "All additional visualizations created.\n",
      "\n",
      "\n",
      "Generating Final Summary\n",
      "Comprehensive Research Summary:\n",
      "LLM Stability and Internal Consistency Study -\n",
      "\n",
      "Experiment ID: 20251217_163437\n",
      "Analysis Date: 2025-12-17 21:53:58\n",
      "\n",
      "\n",
      "Key Findings:\n",
      "\n",
      "1. Overall Stability\n",
      "   • Claim Stability Score (CSS): 0.453\n",
      "     → LLMs maintain ~45.3% semantic overlap\n",
      "   • Stance Volatility Index (SVI): 0.483\n",
      "     → Models flip stance in ~48.3% of cases\n",
      "   • Definition Drift Score (DDS): 0.395\n",
      "     → Definitions drift by ~39.5%\n",
      "\n",
      "2. Model Comparison\n",
      "\n",
      "   deepseek-r1:7b:\n",
      "   • CSS: 0.442 (±0.130)\n",
      "   • SVI: 0.508 (±0.143)\n",
      "   • DDS: 0.382 (±0.137)\n",
      "\n",
      "   llama3:latest:\n",
      "   • CSS: 0.464 (±0.113)\n",
      "   • SVI: 0.458 (±0.170)\n",
      "   • DDS: 0.407 (±0.074)\n",
      "\n",
      "3. Statistical Significance\n",
      "\n",
      "   claim_stability_score:\n",
      "   • Difference: -0.023\n",
      "   • p-value: 0.5923 ns\n",
      "   • Effect size: -0.191\n",
      "\n",
      "   stance_volatility_index:\n",
      "   • Difference: 0.050\n",
      "   • p-value: 0.3550 ns\n",
      "   • Effect size: 0.332\n",
      "\n",
      "   definition_drift_score:\n",
      "   • Difference: -0.025\n",
      "   • p-value: 0.5125 ns\n",
      "   • Effect size: -0.234\n",
      "\n",
      "4. Category Insights\n",
      "\n",
      "   Most Stable: TEMPORAL_CAUSAL (CSS: 0.550)\n",
      "   Least Stable: META_REASONING (CSS: 0.319)\n",
      "   Stability Range: 0.231\n",
      "\n",
      "5. Inflation Paradox\n",
      "   • High factual stability: CSS = 0.578\n",
      "   • High stance volatility: SVI = 0.750\n",
      "\n",
      "\n",
      "Outputs Generated.\n",
      "\n",
      "Data collection: 510/510 responses\n",
      "Semantic metrics calculated.\n",
      "Statistical tests performed.\n",
      "Qualitative examples extracted.\n",
      "Publication-ready tables exported.\n",
      "Comprehensive visualizations created.\n",
      "\n",
      "\n",
      "End of Summary.\n",
      "\n",
      "Final summary saved to: experiment_outputs/analysis/final_summary_20251217_163437.txt\n",
      "\n",
      "\n",
      "Complete Analysis Finished.\n"
     ]
    }
   ],
   "source": [
    "analysis_results = run_complete_analysis() # Comprehensive Pipeline"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
